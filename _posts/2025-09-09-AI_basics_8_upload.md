---
layout: single
title: "AI 입문 8편: 트랜스포머의, Seq2Seq, 타 아키텍쳐와 비교"
categories: "AI"
tag: "Explanation"
toc: true
author_profile: false
sidebar:
nav: "docs"
---

# AI 입문 시리즈

7편까지에서 트랜스포머의 핵심 구성요소(임베딩, 어텐션, FFN, 잔차, LayerNorm, 포지셔널 인코딩)를 정리했다.  
본 8편에서는 트랜스포머의 전체 특징과 Seq2Seq 관점, 대표 모델(BERT/GPT/T5) 정리, 전통적 구조와의 비교를 수식 중심으로 상세히 다룬다.  
추가로 수치 예제의 구체적 계산(행렬 값 전개), PyTorch 기반 Transformer Seq2Seq 구현 및 학습 루프, 빔 서치 및 샘플링 코드를 포함해 실전에서 바로 참고할 수 있도록 했다.  

---

## 0. 이번 편의 핵심

* 트랜스포머의 핵심 장점: 완전 병렬화, 전역 의존성 학습, 확장성이다.  
* Seq2Seq 문제는 입력 시퀀스에서 출력 시퀀스를 생성하는 조건부 확률 문제이다. 트랜스포머는 인코더-디코더 구조로 이를 모델링한다.  
* 인코더 전용(BERT), 디코더 전용(GPT), 인코더+디코더(T5/BART)의 설계 목적이 서로 다르다. 적용 목적에 맞게 구조를 선택한다.  
* RNN/LSTM/CNN과의 주요 차이는 병렬성, 문맥 범위, 계산 복잡도에 있다.  
* 실무 핵심: 마스크·패딩 처리, 학습 스케줄링(warm-up), mixed-precision, 샤딩 전략, 디코더 디코딩(빔/샘플링) 설계 등이다.  

---

## 1. 트랜스포머의 전체 특징

### 1.1 병렬 처리와 연산 구조

트랜스포머는 입력 시퀀스 전체에 대해 동시에 연산을 수행한다.  
Self-Attention 연산은 모든 쿼리-키 쌍의 유사도를 계산하므로 시퀀스 길이 N에 대해 $O(N^2)$ 연산과 $O(N^2)$ 메모리(어텐션 점수 저장)가 발생한다.  
그러나 각 토큰 처리는 독립적 병렬로 수행되므로 GPU/TPU에서 높은 처리량을 달성한다.  

수식으로 한 헤드의 연산은 다음과 같다.  

$$
Q = XW_Q,\quad K = XW_K,\quad V = XW_V
$$

$$
\text{Attn} = softmax\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

여기서 Q,K,V의 계산은 행렬 곱으로 병렬화 가능하다.  
다중 헤드(h개)를 사용하면 서로 다른 부분공간에서 병렬로 관계를 학습한다.  

### 1.2 장기 의존성 처리  

어텐션 가중치는 한 토큰이 시퀀스 내 다른 모든 토큰을 직접 참조하게 한다.  
따라서 장기 의존성(긴 거리의 관련성)을 전역적으로 포착할 수 있다.  
전통적인 순차 모델의 경우, 긴 거리의 정보가 여러 단계의 반복을 통해 전달되어 기울기 소실 문제가 발생하기 쉽다.  

### 1.3 확장성과 파라미터 구성

트랜스포머의 파라미터는 주로 두 곳에서 집중된다:  
어텐션의 투영 행렬(Q,K,V,Output)과 FFN의 두 개의 밀집층(W1, W2). 모델 확장은 d_model, d_ff, h, L(층 수)을 키로 조정한다.  
대형 모델로 확장할 때 파라미터와 연산량이 급증하므로 메모리 통신 최적화와 효율적 샤딩 전략이 필요하다.  

### 1.4 단점과 개선 기법  

* O(N^2) 연산/메모리로 매우 긴 시퀀스에 비효율적이다. 해결책으로 희소 어텐션(sparse attention), 로컬 윈도우, 롱폼(Longformer), 리니어 어텐션 계열이 개발되었다.  
* 표준 어텐션은 위치 정보를 반영하지 않으므로 포지셔널 인코딩이 필수이다(7편).  
* 대형 모델의 학습 비용이 높아 효율적 트레이닝 알고리즘(예: FlashAttention, 혼합 정밀도, 체크포인팅)을 사용한다.  

---

## 2. Seq2Seq(Sequence-to-Sequence) 정의와 수식 전개  

### 2.1 문제 정의  

Seq2Seq 문제는 입력 시퀀스 $X = (x_1,\dots,x_N)$를 받아 출력 시퀀스 $Y = (y_1,\dots,y_M)$를 생성하는 것이다.  
조건부 확률 분해는 다음과 같다.  

$$
P(Y|X) = \prod_{m=1}^M P(y_m | y_{<m}, X)
$$

트랜스포머는 이 확률을 디코더를 통해 모델링한다.  
학습은 정답 시퀀스를 오른쪽으로 한 칸 쉬프트한 입력(teacher forcing)을 사용해 병렬화된 손실 계산을 가능하게 한다.  

### 2.2 학습 손실 함수  

교차 엔트로피 손실을 위치별로 합산·평균한다.  

$$
L = -\frac{1}{M} \sum_{m=1}^M \sum_{v=1}^V y_{m,v} \log \hat{p}_{m,v}
$$

여기서 $\hat{p}_m$은 $softmax(W_o \hat{y}_m + b_o)$이다.  
softmax와 크로스 엔트로피를 함께 사용하면 미분이 간단해진다(softmax+NLL 합성의 미분 성질).  

### 2.3 디코더의 생성 전략  

디코더는 학습 단계와 추론 단계에서 동작 방식이 다르다.  

* 학습: teacher forcing으로 Y_{shifted}를 입력해 전체 위치의 손실을 병렬 계산한다.  
* 추론: 이전에 예측한 토큰을 다시 입력으로 사용해 토큰을 하나씩 생성한다. 생성 전략은 다음과 같다.  

  * 탐욕적(greedy) 디코딩: 각 단계에서 최고 확률 토큰 선택.  
  * 빔 서치(beam search): 여러 후보 시퀀스 병행 유지로 전역 확률이 높은 시퀀스를 탐색한다. 빔 크기 B는 품질-속도 트레이드오프를 결정한다.  
  * 확률적 샘플링: 온도(temperature), top-k, top-p(nucleus) 샘플링으로 다양성 제어.  

### 2.4 미분 흐름과 기울기 전달  

출력층의 미분은 소프트맥스-크로스 엔트로피 조합으로 간단히 계산된다.  
디코더 내부와 인코더로의 기울기 전파는 7편에서 설명한 잔차와 LayerNorm의 야코비안이 포함된다.  
인코더-디코더 MHA를 통해 디코더의 기울기가 인코더의 투영 행렬 및 입력 임베딩으로 전달된다.  

---

## 3. 인코더/디코더 조합별 모델 정리  

각 조합은 학습 목적과 사용 사례가 다르다.  
수식과 학습 목표를 중심으로 정리한다.  

### 3.1 인코더 전용 모델: BERT 계열  

* 구성: 인코더 블록 L개 쌓음. 디코더 없음.  
* 학습 목표: 마스크 언어 모델링(Masked Language Modeling, MLM). 임의 위치의 토큰을 마스킹하고 이를 예측하도록 학습한다.  

수식: 마스크된 위치 i에 대해  

$$
P(w_i | X_{\setminus i}) = softmax(W_o Z_i + b_o)
$$

MLM 손실:  

$$
L_{MLM} = -\sum_{i \in \mathcal{M}} \log P(w_i | X_{\setminus i})
$$

특징:  

* 양방향 문맥(왼쪽과 오른쪽 모두 사용)으로 표현력 강화.  
* 파인튜닝으로 분류, 질의응답, 문장 유사도 등 다양한 다운스트림 태스크에 활용.  

### 3.2 디코더 전용 모델: GPT 계열  

* 구성: 디코더 블록 L개, 자기회귀 구조.  
* 학습 목표: 다음 토큰 예측(Autoregressive Language Modeling).  

손실:  

$$
L_{LM} = -\sum_{t=1}^T \log P(y_t | y_{<t})
$$

특징:  

* 단방향(좌→우) 컨텍스트로 학습되어 생성 품질이 우수.  
* 프롬프트 기반 텍스트 생성, 대화, 코드 생성 등에 강점.  

### 3.3 인코더+디코더 모델: T5, BART 등  

* 구성: 인코더 L_e개 + 디코더 L_d개.  
* 학습 목표: 다양한 텍스트 변환 태스크(번역, 요약, 문장 재구성 등을 하나의 프레임으로 통합).  

수식(일반적 Seq2Seq):  

$$
Z = \mathrm{Encoder}(X),\quad \hat{Y} = \mathrm{Decoder}(Y_{shifted}, Z)
$$

$$
L = -\sum_{m=1}^M \log P(y_m | y_{<m}, Z)
$$

특징:  

* 입력과 출력이 다른 길이를 가질 수 있는 문제에 유연.  
* 디노이즈(노이즈 문장 복원) 혹은 특정 태스크 지시어(예: T5의 "translate to ...")로 범용성 확보.  

---

## 4. 전통적 구조(RNN/LSTM/CNN)과의 비교 (연산·메모리·표현력)  

### 4.1 RNN(순환 신경망)  

* 수식: $h_t = f(W_h h_{t-1} + W_x x_t + b)$.  
* 병렬성: 불가(시간 단계별 의존성), 연산 복잡도: O(N d_h^2) 대역폭.  
* 장기 의존성: 체인룰에 의한 기울기 곱으로 소실 위험 발생.  

적용: 시퀀스 길이가 짧고 순차적 패턴이 중요한 태스크에서 유용하나, 병렬 처리와 장기 의존성에서 제약이 있다.  

### 4.2 LSTM(장기 단기 기억망)  

* 게이트 구조로 정보 흐름을 선택적으로 유지/삭제.  

* 수식(요약):  
  $f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$,  
  $i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$,  
  $c_t = f_t \odot c_{t-1} + i_t \odot \tanh(W_c [h_{t-1}, x_t] + b_c)$,  
  $h_t = o_t \odot \tanh(c_t)$.  

* 병렬성: 제한적, 시퀀스 길이에 선형 의존.  

* 적용: 장기 의존성 개선, 작은 데이터셋에서 효율.  

### 4.3 CNN(합성곱 신경망)  

* 수식: $y[i] = \sum_k W[k] x[i-k]$.  
* 지역적 패턴 캡처(필터 크기 k), 병렬성 양호.  
* 장기 의존성: 깊은 층 또는 큰 필터·다중 스케일 설계 필요.  

적용: 로컬 패턴(음성, 이미지, n-gram) 캡처에 강하고 연산 효율이 높다.  

### 4.4 요약 비교표(정성적)  

* 병렬 처리: 트랜스포머 > CNN > LSTM/RNN.  
* 글로벌 문맥: 트랜스포머 > LSTM > CNN.  
* 메모리(시퀀스 길이 의존): 트랜스포머(O(N^2)) > RNN/LSTM(O(N)) > CNN(O(N k)).  
* 구현 복잡도: 트랜스포머(어텐션/포지션 처리 필요) > RNN/LSTM(게이트) ≈ CNN.  

---

## 5. 수치 예제(확장된 단계별 계산)  

다음은 행렬 수준의 연산을 실제 숫자로 전개해 어텐션과 FFN, 손실 계산을 명시적으로 확인하는 과정이다.  
예시는 N=3, d_model=4 환경을 사용한다.  
여기서는 행렬 계산을 단계별로 모두 전개한다.  
필요 시 실제 수치 계산(숫자 행렬)까지 전개해 제공 가능하다.  

### 수치 예제 — 행렬 값 전개 (정밀 계산)  

목표: 트랜스포머 내부의 연산(간단화된 FFN 예)과 역전파를 실제 숫자로 따라가 본다.  
설정은 다음과 같다.  

* 배치 크기 N = 2 (토큰 2개), d_model = 2, d_ff = 4.  
* 활성화는 설명의 단순화를 위해 ReLU를 사용한다(실전에서는 GELU 권장).  

입력과 가중치는 다음과 같다.  

```text
X = [[1, 2],
     [3, 4]]   # shape (2,2)

W1 = [[0.1, 0.2],
      [0.3, 0.4],
      [0.5, 0.6],
      [0.7, 0.8]]  # shape (4,2)

b1 = [0,0,0,0]

W2 = [[0.1, 0.2, 0.3, 0.4],
      [0.5, 0.6, 0.7, 0.8]]  # shape (2,4)

b2 = [0,0]
```

### 순전파(Forward)  

1. 첫 선형 변환: $Z1 = X @ W1^T$

```text
Z1 = [[0.5, 1.1, 1.7, 2.3],
      [1.1, 2.5, 3.9, 5.3]]
```  

(예: Z1[0,0] = 1*0.1 + 2*0.2 = 0.5)  

2. 활성화: A = ReLU(Z1)  

```text
A = Z1  # 모든 원소 양수
```

3. 두 번째 선형 변환: Out = A @ W2^T + b2  

```text
Out = [[1.70, 3.94],
       [3.90, 9.02]]
```  

(예: $Out[0,0] = 0.5*0.1 + 1.1*0.2 + 1.7*0.3 + 2.3*0.4 = 1.7$)  

### 역전파(Backward) — 단순 가정  

출력에 대한 손실 기울기를 모두 1로 가정한다: dOut = [[1,1],[1,1]].  
이로부터 가중치와 입력에 대한 기울기를 계산한다.  

1. $dW2 = dOut^T @ A$  

```text
dW2 = [[1.6, 3.6, 5.6, 7.6],
       [1.6, 3.6, 5.6, 7.6]]
```

2. $dB2 = sum over batch of dOut rows -> [2,2]$  

3. $dA = dOut @ W2$  

```text
dA = [[0.6, 0.8, 1.0, 1.2],
      [0.6, 0.8, 1.0, 1.2]]
```

4. $dZ1 = dA * ReLU'(Z1) (ReLU' = 1 for Z1>0)$  

```text
dZ1 = dA  # 동일
```

5. $dW1 = (dZ1)^T @ X$  

```text
dW1 = [[2.4, 3.6],
       [3.2, 4.8],
       [4.0, 6.0],
       [4.8, 7.2]]
```

6. $dB1 = sum rows of dZ1 -> [1.2, 1.6, 2.0, 2.4]$

7. $dX = dZ1 @ W1$

```text
dX = [[1.64, 2.00],
      [1.64, 2.00]]
```

요점 요약:  

* 배치 단위 행렬 미분은 $(dL/dZ)^T @$ 입력 형태로 정리된다.  
* 활성화의 미분이 0이 되면 기울기 경로가 차단될 수 있으므로 활성화 선택과 초기화가 중요하다.  

### PyTorch 예제: Transformer Seq2Seq 구현과 학습 루프 (요약)  

다음은 실전에서 바로 실행 가능한 간단한 구현 요약이다. 전체 코드는 위의 PositionalEncoding 및 Seq2SeqTransformer 래퍼를 사용한다.  

핵심 포인트:  

* 입력/출력은 (seq_len, batch) 형태로 정렬한다.  
* 임베딩 후 d_model 스케일링을 적용한다.  
* tgt_mask는 상삼각 마스크, padding masks는 (batch, seq_len) 형태로 생성한다.  
* loss는 CrossEntropyLoss(ignore_index=pad_token)를 사용한다.  
* 학습 시 gradient clipping, mixed precision(권장)을 적용한다.  

(자세한 코드는 문서 본문에 포함된 코드 블록을 참조)  

---

## 6. 확장 기법과 최적화  

### 6.1 어텐션 효율화  

* 희소 어텐션: 모든 쌍이 아니라 일부 쌍만 계산해 O(N·s)로 복잡도 감소.  
* 윈도우 어텐션: 로컬 윈도우 내에서만 어텐션을 수행.  
* 롱시퀀스 모델: Longformer, Performer, Linformer 등 다양한 근사 기법 존재.  

### 6.2 모델 병렬화와 샤딩  

* 파라미터 서버 없는 데이터 병렬화, 모델 병렬화(파이프라인·텐서 샤딩), ZeRO와 같은 옵티마이저-레벨 샤딩을 사용해 메모리 확장.  

### 6.3 학습 안정화  

* 옵티마이저: Adam/AdamW 계열이 표준이다.  
* 학습률 스케줄: warm-up + 역제곱 스케줄 또는 cosine decay.  
* 혼합 정밀도(AMP): 메모리 절감 및 연산 가속.  
* gradient clipping 및 gradient accumulation 사용.  

---  

## 7. 디코딩 전략(자세히)  

### 7.1 탐욕적 디코딩  

매 단계에서 argmax를 선택한다.  
빠르지만 지역 최적에 갇힐 위험이 있다.  

### 7.2 빔 서치  

빔 크기 B를 유지하면서 각 단계에서 B개의 후보 확장 후 확률 기반으로 상위 B개 선택한다.  
빔 길이 제한, 길이 보정(length penalty), 다양성 패널티를 조합해 품질을 제어한다.  

### 7.3 확률적 샘플링  

온도 조절(temperature)로 분포 평탄화, top-k/top-p로 확률질량을 제한해 품질과 다양성 균형을 맞춘다.  

### 디코더 디코딩: 빔 서치와 샘플링 (요약)  

* 빔 서치는 후보 시퀀스를 유지하며 확장해 최종 확률이 높은 시퀀스를 탐색한다. 길이 보정 및 반복 토큰 억제 등이 품질에 큰 영향이 있다.  
* top-k 및 top-p 샘플링은 다양성과 품질을 제어하는 실용적 수단이다. 온도(temperature)와 결합해 출력 분포를 평탄화하거나 날카롭게 조정할 수 있다.  

---

## 8. 시각화 및 디버깅 포인트  

* Attention map: 토큰 간 상관 행렬 시각화로 이상 동작 탐지.  
* Gradient norm: 기울기 폭주/소실 탐지.  
* Perplexity와 학습 손실 추세: 모델 수렴 확인.  
* 마스크와 패딩: 마스크 오류가 전체 학습을 망칠 수 있으므로 자동화된 검사 필요.  

---

## 9. 실무 권장 설정  

* 초깃값: d_model=512, d_ff=2048, h=8, L=6~12로 시작한다.  
* 배치 크기: GPU 메모리에 따라 가능한 크게 설정하고 learning rate를 배치 크기에 맞게 스케일한다.  
* 체크포인팅: 장시간 교육 시 정기 체크포인트와 학습률 스케줄 로그를 보관한다.  
* 작은 데이터셋: 전이 학습을 적극 활용한다.  
* 긴 시퀀스: Longformer/Performer/Local Attention 등 대체 어텐션 기법 검토.  
* 대규모 학습: ZeRO/파이프라인 샤딩/AMP/gradient checkpointing 조합 권장.  

---

## 10. 요약  

* 트랜스포머는 병렬 처리와 전역 문맥 모델링으로 Seq2Seq와 자연어 처리의 많은 문제를 효율적으로 해결한다.  
* 인코더/디코더 조합에 따라 BERT, GPT, T5 계열과 같은 서로 다른 목적의 아키텍처가 등장했다.  
* 전통적 모델과 비교할 때 장단점이 명확하므로 문제 유형에 따라 적절한 아키텍처를 선택해야 한다.  

---

## 결론  

이번 편에서는 트랜스포머의 넓은 그림을 정리했다.  
다음 9편에서는 트랜스포머 최적화 기법(AdamW, FlashAttention, ZeRO, mixed precision)을 다룰 예정이다.  
