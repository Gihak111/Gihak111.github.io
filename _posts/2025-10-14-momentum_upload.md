---
layout: single
title:  "Momentum"
categories: "AI"
tag: "linear algebra"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---


## Momenyum
경사하강법의 한계를 보완하기 위해 존재한다.  
경사하강법은, 현재 위치에서 기울기가 가장 가파은 곳으로만 이동한다.  
이러면, 효율이 박살날 수 있는데,  
예를들면 지그재그로 가버리거나, 지역 최솟값으로 수렴하는 문제가 있다.  
이런 문제를 해결하기 위해 관성의 개념을 도입한 것이 모멘텀이다.  

관성은, 나아가는 방향으로 계속 나아가려고 하는 성질인데,  
이를 응용해 다음과 같은 성질을 가지게 한다.  

 - 학습가속: 과거부터 계속 같은 방향으로 이동했다면 (언덕을 계속 내려왔다면), 관성이 붙어 더 큰 폭으로, 더 빠르게 이동(에타가 커진다)  
 - 진동 감소 및 방향 유지: 이동 방향이 계속 바뀐다면 (지그재그 상황), 과거의 관성이 현재의 급격한 방향 전환을 일부 상쇄시켜 부드럽게 움직이도록 만든다(에타가 작아짐). 덕분에 전체적으로 최적점을 향해 꾸준히 나아갈 수 있다.  
 - 지역 최솟값 탈출: 설령 지역 최솟값에 도달해 기울기가 0이 되더라도, 이전에 쌓아온 관성의 힘으로 그 지점을 통과하여 더 깊은 최적점을 찾아 나설 가능성이 생긴다(에타 소폭 상승).  

## 1. 모멘텀 누적  

$$v_t = \gamma v_{t-1} + \eta \nabla_W L(W)$$  

이 식은 현재 시점($t$)의 속도($v_t$)를 계산하는 과정이다.  
'속도'는 이동할 방향과 크기를 의미이다.  

 - $v_t$: 현재 스텝에서 이동할 방향과 크기를 나타내는 속도 벡터(velocity vector)이다.  
 - $\gamma$ (감마, 모멘텀 계수): '관성'을 얼마나 유지할지 결정하는 값이다. 보통 0.9와 같이 0에 가까운 큰 값을 사용한다. 만약 $\gamma=0.9$라면, 이전 속도를 90% 유지하겠다는 의미이다. $\gamma$가 0이면 그냥 기본적인 경사 하강법과 같다.  
 - $v_{t-1}$: 바로 이전 스텝($t-1$)의 속도, 즉 과거의 이동 방향과 크기이다.  
 - $\eta$ (에타, 학습률): 현재 기울기를 얼마나 반영할지 결정하는 값  
 - $\nabla_W L(W)$: 현재 위치($W$)에서의 손실 함수(Loss Function)의 기울기(Gradient)입니다. 즉, 지금 당장 나아가야 할 방향을 의미한다.  

따라서, 현재 이동할 방향과 크기($v_t$)는 과거의 관성($\gamma v_{t-1}$)에 현재의 기울기 정보($\eta \nabla_W L(W)$)를 더해서 결정된다는 거다.  


## 2. 가중치 업데이트  

$$W \leftarrow W - v_t$$  

이 식은 계산된 속도($v_t$)를 이용해 모델의 가중치($W$)를 실제로 업데이트(이동)하는 과정이다.  
현재 가중치($W$)에서 위에서 계산한 최종 이동 방향과 크기($v_t$)만큼 빼주어 가중치를 업데이트한다.  

## 3. 종합
과정을 전부 종합해 보면,  
1.  시작: 처음에는 관성이 없으므로($v_0=0$), 일반 경사 하강법처럼 현재 기울기 방향으로만 이동한다.
2.  이동하며 관성 축적: 이동을 반복하면서, 속도 벡터($v$)에 과거의 이동 방향과 크기가 점차 누적된다.
3.  가속 및 방향 유지: 만약 기울기 방향이 계속 비슷하다면, $v$는 점점 커져서 더 빠르게 이동하게 된다. 방향이 계속 바뀐다면, 과거의 관성이 급격한 방향 전환을 막아주어 전체적으로 부드럽고 일관된 방향으로 나아가게 된다.  

## 결론
기존 경사하강법의 한계를 잘 넘어선 그런 느낌이다.  
이거 쓰면 더 빠로고 안정적인 딥러닝 모델학습이 가능해 진다.  