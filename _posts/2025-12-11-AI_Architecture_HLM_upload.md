---
layout: single
title: "AI 아키텍쳐 6. HLM"
categories: "AI"
tag: "Architecture"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## HLM
Hierarchical Latent Models은 단일 잠재 공간에 의존하던 기존 생성 모델의 한계인 '의미적 구조와 미세 디테일의 불균형'을 해결하기 위해 제안된 프레임워크이다.  
기존 모델들은 모든 정보를 하나의 잠재 벡터에 압축하려다 보니, 복잡한 장면을 생성할 때 전체적인 구도가 무너지거나 텍스처가 뭉개지는 현상이 발생하곤 했다.  
HLM은 데이터를 여러 계층(Hierarchy)의 잠재 변수로 분해하여, 상위 계층에서는 전역적인 구조를, 하위 계층에서는 국소적인 디테일을 담당하게 함으로써 생성 품질과 제어 능력을 비약적으로 향상시켰다.  

## 2. 수식적 원리: 계층적 분해와 조건부 확률
HLM의 이론적 토대는 잠재 변수의 계층적 조건부 확률 분포이다.  
데이터 $x$를 생성하기 위해 단일 잠재 변수 $z$ 대신, 추상화 단계가 다른 일련의 잠재 변수 $z = \{z_1, z_2, ..., z_L\}$을 도입한다.  

생성 과정은 가장 추상적인 상위 레벨 $z_L$에서 시작하여 하위 레벨로 순차적으로 진행되는 연쇄 법칙(Chain Rule)으로 표현된다.  

$$p_\theta(x) = \int p_\theta(x|z_1) \prod_{l=1}^{L-1} p_\theta(z_l|z_{l+1}) p(z_L) dz_{1:L}$$

여기서 $p_\theta(z_l|z_{l+1})$는 상위 레벨의 잠재 변수 $z_{l+1}$이 주어졌을 때 하위 레벨 $z_l$을 생성하는 조건부 분포이다.  
기존 모델이 $p(x|z)$를 한 번에 학습했다면, HLM은 이 복잡한 문제를 여러 개의 **단순한 조건부 생성 문제(Sub-problems)**로 쪼개어 해결한다.  

## 3. 학습 방법론: Nested Training과 Stage-wise Optimization
HLM은 각 계층이 서로 다른 수준의 특징(Feature)을 학습하도록 유도해야 한다.  
이를 위해 **Nested Training** 혹은 **Stage-wise Optimization** 전략을 사용한다.  

학습 손실 함수는 각 계층별 ELBO(Evidence Lower Bound)의 합으로 구성된다.  

$$\mathcal{L}_{HLM} = \mathcal{L}_{recon}(x, z_1) + \sum_{l=1}^{L-1} \mathbb{E}_{z_{l+1}} [D_{KL}(q(z_l|z_{l+1}) || p_\theta(z_l|z_{l+1}))] + D_{KL}(q(z_L) || p(z_L))$$

1.  **Top-Down Guidance**: 상위 계층 모델은 이미지의 대략적인 윤곽과 의미적 배치(Semantic Layout)를 학습한다.  
2.  **Bottom-Up Refinement**: 하위 계층 모델은 상위 계층의 가이드를 받아 구체적인 픽셀 값과 고주파수(High-frequency) 정보를 채워 넣는다.  
3.  **최적화**: 모든 계층을 동시에 학습(Joint Training)하거나, 안정성을 위해 상위 계층부터 순차적으로 고정(Freeze)하며 학습하는 방식을 취한다.  

## 4. 핵심 기술: 시맨틱 인코더와 잔차 학습
HLM이 고품질 이미지를 생성할 수 있는 비결은 두 가지 핵심 기술에 있다.  

* **계층적 시맨틱 인코더 (Hierarchical Semantic Encoder)**:  
단순히 이미지를 압축하는 것이 아니라, 사전 학습된 비전 모델(ViT 등)을 활용해 각 계층에 맞는 '의미론적 특징'을 추출하여 잠재 공간에 매핑한다.  
이는 $z_L$이 단순한 노이즈가 아닌, "강아지가 잔디 위에 있다"와 같은 고차원 정보를 담도록 강제한다.  
* **잔차 생성 (Residual Generation)**:  
하위 계층은 상위 계층에서 이미 생성된 정보와의 차이(Residual)만을 예측하도록 설계된다.  
이는 모델이 불필요한 중복 연산을 피하고, 미세한 디테일 보정에만 집중할 수 있게 하여 학습 효율을 높인다.  

## 5. 아키텍처 확장: HLM-Control
HLM의 계층적 구조는 제어 가능한 생성(Controllable Generation)에 특화되어 있다.  
이를 활용한 **HLM-Control** 아키텍처는 사용자의 개입을 각 계층별로 분리하여 허용한다.  

$$z_l^{new} = \mathcal{M}(z_l^{pred}, c_{user})$$

* **구조 제어**: 사용자가 스케치나 뎁스 맵(Depth Map)을 제공하면, 이는 상위 계층 $z_L, z_{L-1}$의 생성 과정에만 관여하여 전체 구도를 잡는다.  
* **스타일 제어**: 텍스처나 색감에 대한 프롬프트는 하위 계층 $z_1, z_2$에 주입되어, 구조를 해치지 않으면서 스타일만 변경하는 것이 가능하다.  
이러한 특성 덕분에 HLM은 복잡한 인페인팅(Inpainting)이나 스타일 전이 작업에서 기존 모델보다 월등한 일관성을 보인다.  

## 6. 추론 및 Cascaded Sampling
HLM의 추론 과정은 상위 레벨에서 하위 레벨로 흐르는 **Cascaded Sampling** 방식을 따른다.  

1.  **Global Sampling**: 가장 압축된 잠재 공간 $z_L$에서 전체적인 의미 구조를 샘플링한다.  
2.  **Progressive Decoding**: $z_{l+1}$을 조건(Condition)으로 하여 $z_l$을 생성한다. 이 과정에서 각 단계별로 별도의 가이던스(Guidance)를 적용할 수 있다.  
3.  **Final Reconstruction**: 최하위 잠재 변수 $z_1$을 픽셀 공간으로 디코딩하여 최종 이미지를 얻는다.  

이 방식은 단일 단계 모델보다 추론 단계가 많아질 수 있지만,  
각 단계의 잠재 공간 크기가 작아 연산량은 효율적으로 관리된다.  

## 결론
HLM은 "모든 것을 한 번에 해결하려는" 기존 생성 모델의 접근법을 버리고, 나누어 정복하는 계층적 접근을 취한 아키텍처이다.  
복잡한 장면 구성 능력과 정교한 디테일 표현력을 동시에 확보했다는 점에서 학술적으로 매우 고무적인 성과라 할 수 있다. 특히 3D 생성이나 비디오 생성처럼 구조적 일관성이 중요한 분야에서 그 진가가 발휘된다.  
하지만 아키텍처가 복잡하고 학습 파이프라인 구축이 까다롭다는 단점이 있어, 실제 서비스 레벨에서는 여전히 무겁게 느껴진다.  
결국 연구실에서는 HLM으로 논문을 쓰지만, 현업에서는 그냥 Schnell나 미드저니 쓴다.  
그게 제일 빠르고 잘 나온다.  