---
layout: single
title:  "분류시 델타 계산"
categories: "AI"
tag: "linear algebra"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## 분류 문제에서 델타(δ) 계산 정리  

딥러닝에서 역전파를 수행할 때 출력층의 델타(δ)는 손실 함수의 미분을 통해 얻어진다.  
델타를 계산할 때 가장 많이 혼동되는 부분은 “예측값 $\hat{y}$에서 실제값 $y$를 뺄지, 아니면 반대로 할지”이다.  


## 회귀 문제 (MSE 기준)  

회귀 문제에서 흔히 사용하는 손실 함수는 평균제곱오차(Mean Squared Error, MSE)이다.  

$$
L = \frac{1}{2}(y - \hat{y})^2
$$

이를 예측값 $\hat{y}$에 대해 미분하면,  

$$
\frac{\partial L}{\partial \hat{y}} = \hat{y} - y
$$

따라서 MSE를 사용할 경우 출력층의 델타는  

$$
\delta = \hat{y} - y
$$

이다.  


## 분류 문제 (Softmax + Cross-Entropy 기준)  

분류 문제에서 일반적으로 사용하는 조합은 소프트맥스(Softmax) 출력과 크로스 엔트로피(Cross-Entropy) 손실이다.  

손실 함수는 다음과 같다.  

$$
L = - \sum_{i} y_i \log(\hat{y}_i)
$$

여기서  

* $y$는 원-핫(One-hot) 인코딩된 실제 라벨 벡터이다.  
* $\hat{y}$는 소프트맥스 출력으로, 각 클래스에 대한 예측 확률 벡터이다.  

이를 소프트맥스 입력값 $z$에 대해 미분하면,  

$$
\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i
$$

따라서 분류 문제에서도 출력층의 델타는  

$$
\delta = \hat{y} - y
$$

이다.  


## 혼동이 생기는 이유  

일부 교재에서는 단순히 **오차(error)** 를  

$$
(y - \hat{y})
$$

라고 정의한다.  
반면 실제 역전파에서 사용하는 델타는 손실 함수의 미분 결과로서  

$$
(\hat{y} - y)
$$

이다.  

즉, "오차(error)"와 "델타(delta)"라는 용어를 혼용하면 부호가 반대로 보이는 혼동이 생긴다.  
그러나 역전파에서 중요한 것은 손실 함수의 기울기 방향이므로, 일관성 있게 정의하면 학습은 올바른 방향으로 진행된다.  

---

## 결론  

MSE든 Softmax+Cross-Entropy든 출력층 델타는  

$$
\delta = \hat{y} - y
$$

이다.  
오차를 $(y - \hat{y})$로 정의하는 경우가 있어 혼동이 발생하지만, 실제 역전파에서 사용하는 델타는 반드시 **예측값 - 실제값**이다.  
이제 소프트맥스 + 크로스엔트로피 역전파 과정에서 델타의 부호가 확실히 **$\hat{y} - y$** 라는 점이 명확해진다.  
