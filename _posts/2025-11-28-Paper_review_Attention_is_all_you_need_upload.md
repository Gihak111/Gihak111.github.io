---
layout: single
title:  "[논문 리뷰] Attention Is All You Need"
categories: "AI"
tag: "review"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## 논문 리뷰
지난번 최신 기술 동향에 이어, 오늘은 AI 역사상 가장 큰 충격을 주었던 '근본 중의 근본' 논문을 리뷰해 볼까 한다.  
지금의 ChatGPT, Claude, Llama 같은 거대 언어 모델(LLM)이 탄생할 수 있었던 시발점이자, 자연어 처리의 판도를 완전히 뒤바꾼 전설적인 논문이다.  
워낙 유명한 논문이지만 수식이 많아 어렵게 느껴질 수 있는데, 이번에도 역시 초심자도 이해할 수 있을 정도로 엄청 풀어서 적어보겠다.

Attention Is All You Need  
논문 링크 : [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

이 논문은, 기존의 순환 신경망(RNN, LSTM) 방식이 가지고 있던 느린 학습 속도와 장기 기억 소실 문제를 해결하기 위해, 오직 '어텐션(Attention)' 메커니즘만으로 구성된 새로운 모델인 **트랜스포머(Transformer)**를 제안한다.


과거의 번역기나 언어 모델은 RNN(Recurrent Neural Network)이라는 구조를 주로 사용했다.  
RNN은 사람이 글을 읽는 것처럼, 단어를 처음부터 순서대로 하나씩 처리한다.  
"나는 밥을 먹었다"라는 문장이 있다면 "나는"을 읽고, 그 정보를 가지고 "밥을"을 읽고, 다시 그 정보를 누적해 "먹었다"를 읽는 식이다.  
하지만 이 방식은 두 가지 치명적인 단점이 있다.  
첫째, 문장이 길어지면 앞부분의 내용("나는")을 뒷부분으로 갈수록 잊어버리게 된다.  
둘째, 순서대로 계산해야 하기 때문에, 컴퓨터의 GPU를 여러 개 써서 병렬로 한꺼번에 계산하는 것이 불가능해 학습 속도가 매우 느리다.  
따라서, 구글 연구진은 "순서를 없애고, 문장 전체를 한 번에 보면서 중요한 단어에만 집중(Attention)하면 어떨까?"라는 아이디어를 냈고, 그 결과물이 바로 이 논문의 핵심인 트랜스포머 구조다.  
이 논문에서는 기존의 복잡한 순환 구조를 다 쳐내고, 다음과 같은 혁신적인 방법들을 통해 성능과 속도를 모두 잡았다.


1. 셀프 어텐션 (Self-Attention)  
어텐션은 문장 내의 단어들이 서로 어떤 연관이 있는지를 계산하는 방법이다.  
예를 들어, "The animal didn't cross the street because it was too tired"라는 문장이 있다고 치자.  
여기서 'it'이 가리키는 것이 'animal'인지 'street'인지 컴퓨터는 알기 어렵다.  
기존 RNN은 단어 순서대로만 처리해서 이 관계를 파악하기 힘들었지만, 셀프 어텐션은 문장 내의 모든 단어끼리의 관계 점수를 매긴다.  
'it'과 'animal'의 연관 점수를 높게 계산함으로써, 문맥을 훨씬 더 정확하게 파악할 수 있게 한다.  
즉, 어떤 단어를 처리할 때, 문장 안의 다른 모든 단어들을 훑어보고 중요한 단어의 정보를 더 많이 가져오는 방식이다.


2. 멀티 헤드 어텐션 (Multi-Head Attention)  
한 번만 집중(Attention)하는 것으로는 부족할 수 있다.  
문장에는 문법적인 관계도 있고, 의미적인 관계도 있고, 위치적인 관계도 있기 때문이다.  
그래서 논문에서는 어텐션을 여러 개(Multi-Head)로 나누어 병렬로 수행하는 방법을 제안한다.  
마치 한 문장을 여러 명의 전문가가 동시에 분석하는 것과 같다.  
어떤 헤드(전문가)는 '주어-동사' 관계를 보고, 다른 헤드는 '대명사-명사' 관계를 보는 식이다.  
이렇게 여러 관점에서 문장을 분석한 뒤 합치면(Concat), 훨씬 더 풍부하고 정교한 문맥 이해가 가능해진다.  
이 과정을 통해 모델은 단어 사이의 미묘한 뉘앙스까지 캐치할 수 있게 된다.


3. 포지셔널 인코딩 (Positional Encoding)  
앞서 말했듯, 트랜스포머는 RNN처럼 단어를 순서대로 넣지 않고 한꺼번에 집어넣는다.  
그러다 보니 컴퓨터 입장에서는 "나는 너를 사랑해"와 "너를 나는 사랑해"가 단어 구성이 같아서 똑같이 보일 수 있다.  
순서 정보가 사라졌기 때문이다.  
이를 해결하기 위해 단어 자체의 값에 '위치 정보(번호표)'를 더해주는 것이 포지셔널 인코딩이다.  
각 단어가 문장의 어느 위치에 있는지 수학적인 패턴(사인, 코사인 함수)을 이용해 표시해 줌으로써, 순서대로 처리하지 않아도 문장의 어순을 이해할 수 있게 만들었다.  
이 덕분에 순차적인 계산을 기다릴 필요 없이, 행렬 연산으로 한 번에 병렬처리가 가능해져 학습 속도가 비약적으로 빨라졌다.


이러한 구조를 통해 탄생한 트랜스포머 모델은 당시 번역 작업(WMT 2014)에서 기존 최고 성능을 기록했던 모델들보다 훨씬 높은 점수(BLEU Score)를 기록했다.  
더 놀라운 것은, 기존 모델들이 훈련하는 데 며칠이 걸리던 작업을 트랜스포머는 훨씬 적은 시간과 비용으로 해냈다는 점이다.  
이 논문 이후로 자연어 처리 분야는 완전히 트랜스포머 천하가 되었다.  
우리가 지금 쓰고 있는 GPT(Generative Pre-trained Transformer)의 'T'가 바로 이 트랜스포머를 뜻한다.  
복잡한 순환 고리를 끊어내고 '어텐션'만으로 충분하다는 것을 증명함으로써, 대규모 데이터 학습과 초거대 언어 모델의 시대를 연 역사적인 논문이라 할 수 있다.  
메모리와 계산 효율성을 극대화하면서도 성능은 더 뛰어난, AI 연구자라면 반드시 읽어야 할 필독서다.