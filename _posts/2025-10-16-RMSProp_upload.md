---
layout: single
title:  "RMSProp"
categories: "AI"
tag: ["optimizer", "deep learning"]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## AdaGrad의 문제를 해결하다
AdaGrad는 각 파라미터마다 최적화된 학습률을 제공하는 효율적인 방법이다.  
하지만 학습이 길어질수록 학습률이 0에 가까워져 결국 학습이 멈추는 문제 라는 명확한 단점이 존재한다.  
이는 $G_t$가 과거의 모든 기울기 제곱을 누적하여 더하기 때문에 분모가 계속해서 커지기 때문이다.  

이 문제를 해결하기 위해 등장한 것이 RMSProp (Root Mean Square Propagation)이다.  
RMSProp은 AdaGrad의 핵심 아이디어인 적응적 학습률 개념은 유지하면서, 학습이 멈추는 문제를 효과적으로 해결한다.  

그 해결책은 과거의 모든 기록을 동일하게 누적하지 않고, 최신 기록에 더 큰 가중치를 부여하는 것이다.  


## 1. 기울기 제곱의 지수 이동 평균 (Exponential Moving Average)
AdaGrad가 과거의 모든 기울기 제곱을 단순 합산했다면, RMSProp은 지수 이동 평균(Exponential Moving Average)을 사용해 이 값을 계산한다.  

$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)(\nabla_W L(W))^2$$

위 식의 각 요소는 다음과 같다.  

-   $E[g^2]_t$: 현재 시점($t$)의 기울기 제곱에 대한 '평균적인' 값으로, AdaGrad의 $G_t$를 대체한다.  
-   $\gamma$ (gamma): **감쇠율(decay rate)**. 과거 정보의 영향력을 얼마나 줄일지를 결정하는 계수이며, 일반적으로 0.9와 같은 값을 사용한다.  
-   $E[g^2]_{t-1}$: 이전 시점($t-1$)까지 계산된 '평균적인' 기울기 제곱값이다.  
-   $(\nabla_W L(W))^2$: 현재 시점의 기울기 제곱값이다.  

이 식은 과거의 평균 정보($E[g^2]_{t-1}$)의 영향력은 감쇠율($\gamma$)만큼 줄이고, 새로운 최신 정보($(\nabla_W L(W))^2$)를 더 비중 있게 반영하여 현재의 평균을 갱신한다는 의미를 가진다.  

이를 통해 오래된 기울기 정보의 영향력은 시간이 지남에 따라 점차 감소하므로, 분모가 무한정 커지는 것을 방지할 수 있다.  
이는 곧 학습률이 0으로 수렴하는 문제를 해결하는 핵심 원리가 된다.


## 2. 가중치 업데이트 (Weight Update)
가중치 업데이트 식은 AdaGrad와 거의 동일하며, 분모에 사용되는 값이 $G_t$에서 $E[g^2]_t$로 변경된 것만이 차이점이다.  

$$W \leftarrow W - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \nabla_W L(W)$$

-   $\eta$: 사용자가 설정하는 전역 학습률(Global Learning Rate)이다.  
-   $\sqrt{E[g^2]_t + \epsilon}$: 이 부분이 최신 정보가 더 많이 반영된 '평균적인' 기울기 크기가 된다. 분모가 무한정 커지지 않으므로 학습률이 0으로 떨어지는 현상을 막아준다.  

결과적으로 RMSProp은 AdaGrad와 같이 각 파라미터에 맞는 학습률을 적용하면서도, 학습 후반부에 학습이 정체되는 문제를 해결한 한 단계 발전된 옵티나이저라 할 수 있다.  

## 결론
이를 통해 두 가지 중요한 개념을 정리할 수 있다.  
1.  **AdaGrad**: 각 파라미터마다 다른 학습률을 적용하는 방법론.  
2.  **RMSProp**: AdaGrad의 학습률 소실 문제를 지수 이동 평균 기법으로 해결한 방법론.  
앞서 AdaGrad가 모멘텀과 합쳐져 Adam이 된다고 언급했지만, 보다 정확하게는 RMSProp의 아이디어와 모멘텀(Momentum)이 결합하여 Adam 옵티나이저가 탄생한다.  