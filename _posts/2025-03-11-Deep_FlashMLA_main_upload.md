---
layout: single
title:  "ë”¥ì‹œí¬ FlashMLA-main ì•Œì•„ë³´ê¸°"
categories: "AI"
tag: "code"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## FlashMLA-main
ë”¥ì‹œí¬ê°€ ë¬´ë£Œë¡œ ê³µê°œí•œ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì´ë‹¤.  
ì½”ë“œë¥¼ ë¶„í•´í•˜ì—¬ ì´í•´í•˜ëŠ” ì‹œê°„ì„ ê°€ì ¸ë³´ì.


ë”¥ì‹œí¬ê°€ ë¬´ë£Œë¡œ ê³µê°œí•œ FlashMLAëŠ” Hopper GPUë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ MLA ë””ì½”ë”© ì»¤ë„ë¡œ, ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ë¥¼ ìµœì í™”í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

í˜„ì¬ ê³µê°œëœ ë²„ì „ì—ì„œëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ì§€ì›í•œë‹¤.
- **BF16, FP16** ì§€ì›  
- **Paged kvcache** ë°©ì‹ ì‚¬ìš© (ë¸”ë¡ í¬ê¸°: 64)  
- **CUDA 12.3 ì´ìƒ** ì§€ì› (ê¶Œì¥: 12.8 ì´ìƒ)  
- **PyTorch 2.0 ì´ìƒ** í•„ìš”  

ê³ ì„±ëŠ¥ ì„¤ì •ì—ì„œ **ë©”ëª¨ë¦¬ ëŒ€ì—­í­ 3000GB/s**, **ì—°ì‚° ì„±ëŠ¥ 580 TFLOPS**ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.  

---

## ì„¤ì¹˜

ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆë‹¤.

```bash
python setup.py install
```

ì´í›„, ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ë©´ ëœë‹¤.

```bash
python tests/test_flash_mla.py
```

> ğŸ’¡ CUDA **12.3 ì´ìƒ**ì—ì„œ ë™ì‘í•˜ì§€ë§Œ, **12.8 ì´ìƒ**ì„ ê¶Œì¥í•œë‹¤.  
> ğŸ’¡ VRAMì€ **í”„ë ˆì„ë‹¹ ìµœì†Œ 3GB**ê°€ í•„ìš”í•˜ë‹¤.

---

## ì‚¬ìš©ë²•

ì•„ë˜ ì˜ˆì œëŠ” **FlashMLAë¥¼ í™œìš©í•œ kvcache ê¸°ë°˜ ì—°ì‚°**ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì´ë‹¤.

```python
from flash_mla import get_mla_metadata, flash_mla_with_kvcache

tile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)

for i in range(num_layers):
    ...
    o_i, lse_i = flash_mla_with_kvcache(
        q_i, kvcache_i, block_table, cache_seqlens, dv,
        tile_scheduler_metadata, num_splits, causal=True,
    )
    ...
```

### `scaled_dot_product_attention` í•¨ìˆ˜

FlashMLAì˜ í•µì‹¬ ì—°ì‚° ì¤‘ í•˜ë‚˜ì¸ **Scaled Dot-Product Attention**ì„ êµ¬í˜„í•œ ì˜ˆì œì´ë‹¤.

```python
import torch

def scaled_dot_product_attention(query, key, value, h_q, h_kv, is_causal=False):
    """
    Scaled Dot-Product Attention êµ¬í˜„.

    query: ì¿¼ë¦¬ í–‰ë ¬ (Batch, Head, Seq_len_q, Dim)
    key: í‚¤ í–‰ë ¬ (Batch, Head, Seq_len_k, Dim)
    value: ê°’ í–‰ë ¬ (Batch, Head, Seq_len_v, Dim)
    h_q: ì¿¼ë¦¬ í—¤ë“œ ê°œìˆ˜
    h_kv: í‚¤-ê°’ í—¤ë“œ ê°œìˆ˜
    is_causal: ë¯¸ë˜ í† í° ë§ˆìŠ¤í‚¹ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    """
    d_k = query.shape[-1]
    scores = torch.matmul(query, key.transpose(-2, -1)) / d_k**0.5

    if is_causal:
        seq_len = scores.shape[-1]
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(query.device)
        scores = scores.masked_fill(mask == 1, float('-inf'))

    attn = torch.nn.functional.softmax(scores, dim=-1)
    return torch.matmul(attn, value)
```

ì´ í•¨ìˆ˜ëŠ” FlashMLA ë‚´ë¶€ì˜ **flash_mla_with_kvcache** ì—°ì‚°ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•œë‹¤.

---

## ì½”ë“œ ë¶„ì„

FlashMLAì˜ ì—°ì‚° ë¡œì§ì€ `brench_flash_mla`ì— êµ¬í˜„ë˜ì–´ ìˆë‹¤.  
ì£¼ìš” ì—°ì‚° íë¦„ì„ ì´í•´í•˜ë ¤ë©´ `get_mla_metadata`, `flash_mla_with_kvcache` í•¨ìˆ˜ ë¶„ì„ì´ í•„ìš”í•˜ë‹¤.  

---

## ê´€ë ¨ í”„ë¡œì íŠ¸

FlashMLAëŠ” **FlashAttention 2&3** ë° **NVIDIA Cutlass** í”„ë¡œì íŠ¸ì˜ ì˜í–¥ì„ ë°›ì•˜ë‹¤.

| í”Œë«í¼        | ê³µì‹ ì‚¬ì´íŠ¸ | ê´€ë ¨ FlashMLA ë²„ì „ |
|--------------|-----------|-------------------|
| **MetaX** | [MetaX](https://www.metax-tech.com) | [MetaX-MACA/FlashMLA](https://github.com/MetaX-MACA/FlashMLA) |
| **Moore Threads** | [Moore Threads](https://www.mthreads.com/) | [MooreThreads/MT-flashMLA](https://github.com/MooreThreads/MT-flashMLA) |
| **Hygon DCU** | [Hygon Developer](https://developer.sourcefind.cn/) | [OpenDAS/MLAttention](https://developer.sourcefind.cn/codes/OpenDAS/MLAttention) |
| **Intellifusion** | [Intellifusion](https://www.intellif.com) | [Intellifusion/tyllm](https://gitee.com/Intellifusion_2025/tyllm/blob/master/python/tylang/flash_mla.py) |
| **Iluvatar Corex** | [Iluvatar Corex](https://www.iluvatar.com) | [Deep-Spark/FlashMLA](https://github.com/Deep-Spark/FlashMLA/tree/iluvatar_flashmla) |

---

## ì¸ìš©

FlashMLAë¥¼ ì—°êµ¬ ë˜ëŠ” ë…¼ë¬¸ì—ì„œ í™œìš©í•  ê²½ìš° ì•„ë˜ BibTeXì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```bibtex
@misc{flashmla2025,
      title={FlashMLA: Efficient MLA decoding kernels},
      author={Jiashi Li},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/FlashMLA}},
}
```

## ì£¼ìš”í•¨ìˆ˜

### 1. `get_mla_metadata`  
FlashMLAì˜ íƒ€ì¼ ìŠ¤ì¼€ì¤„ë§ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.   
ì‰½ê²Œ ë§í•´ì„œ, MLA ì—°ì‚°ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ì…ë ¥ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ìª¼ê°¤ì§€(Tiling) ê²°ì •í•˜ëŠ” ì—­í• ì´ë‹¤.  
 
- ì‹œí€€ìŠ¤ ê¸¸ì´(`cache_seqlens`)ì™€ ì¿¼ë¦¬-í‚¤ ê´€ê³„ë¥¼ ê³„ì‚°í•´ì„œ íƒ€ì¼ í¬ê¸°ë‘ ë³‘ë ¬ ì—°ì‚° ê°œìˆ˜(`num_splits`)ë¥¼ ì •í•´ì¤Œ.  
- FlashMLAê°€ ê¸°ì¡´ Attentionë³´ë‹¤ ë¹ ë¥¸ ì´ìœ  ì¤‘ í•˜ë‚˜ê°€ ì´ íƒ€ì¼ë§ êµ¬ì¡° ë•Œë¬¸ì´ë‹¤ë‹¤.  

ì½”ë“œ íë¦„:  
```python
tile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)
```  
- `cache_seqlens`: ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ ì •ë³´  
- `s_q * h_q // h_kv`: ì¿¼ë¦¬ ê¸¸ì´ì™€ í‚¤-ê°’ ê¸¸ì´ ì¡°ì •  
- `h_kv`: í‚¤-ê°’ í—¤ë“œ ê°œìˆ˜  

FlashMLAëŠ” ì—°ì‚°ì„ í•œ ë²ˆì— ë‹¤ í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ íƒ€ì¼ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì‹¤í–‰í•œë‹¤.  
ì´ê±¸ ê²°ì •í•˜ëŠ” ê²Œ `get_mla_metadata` í•¨ìˆ˜ê³ , ì—¬ê¸°ì„œ `num_splits`ê°€ ì–¼ë§ˆë‚˜ ì ì ˆí•˜ê²Œ ì„¤ì •ë˜ëŠ”ì§€ê°€ ì†ë„ì™€ ì„±ëŠ¥ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤.  

---

### 2. `flash_mla_with_kvcache`   
FlashMLAì˜ í•µì‹¬ ì—°ì‚°ì´ ëŒì•„ê°€ëŠ” í•¨ìˆ˜ì´ë‹¤.    
ì¿¼ë¦¬(`query`), í‚¤-ê°’ ìºì‹œ(`kvcache`), ê·¸ë¦¬ê³  ë¸”ë¡ í…Œì´ë¸”(`block_table`)ì„ ë°›ì•„ì„œ ìµœì í™”ëœ MLA ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.  
  
- Paged kvcache êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”  
- ê¸°ì¡´ Attentionë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ ì—°ì‚° ê°€ëŠ¥  
- `causal=True` ì„¤ì • ì‹œ, Auto-Regressive Transformer(GPT ê°™ì€ ëª¨ë¸) ì§€ì›  

ì½”ë“œ íë¦„:  
```python
o_i, lse_i = flash_mla_with_kvcache(
    q_i, kvcache_i, block_table, cache_seqlens, dv,
    tile_scheduler_metadata, num_splits, causal=True,
)
```  
- `q_i`: ì¿¼ë¦¬ í…ì„œ  
- `kvcache_i`: Key-Value ìºì‹œ  
- `block_table`: ë¸”ë¡ êµ¬ì¡° ê´€ë¦¬ í…Œì´ë¸”  
- `cache_seqlens`: ê° ì‹œí€€ìŠ¤ ê¸¸ì´  
- `dv`: Valueì˜ ë³€í™”ëŸ‰  
- `tile_scheduler_metadata, num_splits`: ìœ„ì—ì„œ `get_mla_metadata`ë¡œ ì–»ì€ íƒ€ì¼ ì •ë³´  
- `causal=True`: Decoder-Only Transformer(GPT ê°™ì€ ëª¨ë¸)ì—ì„œ í•„ìˆ˜ ì„¤ì •  
  
FlashMLAì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë¡œì§ì´ ì—¬ê¸° ë“¤ì–´ìˆì–´.  
- ê¸°ì¡´ Attentionì€ ëª¨ë“  í† í°ì„ í•œ ë²ˆì— ê³„ì‚°í•˜ëŠ”ë°, FlashMLAëŠ” Paged kvcacheë¥¼ í™œìš©í•´ ë¶€ë¶„ì ìœ¼ë¡œ ì—°ì‚°í•œë‹¤.  
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ë¹ ë¥´ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì´ìœ ê°€ ì—¬ê¸°ì— ìˆë‹¤.  


### 3. `scaled_dot_product_attention` (ê¸°ë³¸ Attention ë¹„êµìš©)  
ì´ í•¨ìˆ˜ëŠ” FlashMLAë‘ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì€ ì—†ë‹¤.  
í•˜ì§€ë§Œ, ê¸°ì¡´ Attentionì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì•Œê³  ìˆì–´ì•¼ FlashMLAê°€ ì–¼ë§ˆë‚˜ ìµœì í™”ëëŠ”ì§€ ë¹„êµí•  ìˆ˜ ìˆë‹¤.  

ì½”ë“œ íë¦„:  
```python
def scaled_dot_product_attention(query, key, value, h_q, h_kv, is_causal=False):
    d_k = query.shape[-1]
    scores = torch.matmul(query, key.transpose(-2, -1)) / d_k**0.5

    if is_causal:
        seq_len = scores.shape[-1]
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(query.device)
        scores = scores.masked_fill(mask == 1, float('-inf'))

    attn = torch.nn.functional.softmax(scores, dim=-1)
    return torch.matmul(attn, value)
```  
- ê¸°ë³¸ì ì¸ Scaled Dot-Product Attentionì´ ìˆ˜í–‰í•˜ëŠ” ì—°ì‚° ê³¼ì •ì„ ë³¼ ìˆ˜ ìˆë‹¤.    
- `query * key^T` â†’ `softmax` â†’ `value` ì—°ì‚°ì„ ê±°ì¹˜ëŠ” ì „í†µì ì¸ ë°©ì‹ì´ë‹¤.  
 
FlashMLAëŠ” ì´ ê³¼ì •ì„ íƒ€ì¼ë§ê³¼ Paged kvcache ë°©ì‹ìœ¼ë¡œ ìµœì í™”í•´ì„œ, ë©”ëª¨ë¦¬ ì ˆì•½ + ë¹ ë¥¸ ì—°ì‚°ì„ ë™ì‹œì— ê°€ëŠ¥í•˜ê²Œ ë§Œë“ ë‹¤.    



| í•¨ìˆ˜ | ì—­í•  | ì¤‘ìš”í•œ ì´ìœ ìœ  |
|------|------|-------------|
| `get_mla_metadata` | íƒ€ì¼ ìŠ¤ì¼€ì¤„ë§ & ë³‘ë ¬ ì—°ì‚° ì„¤ì • | MLA ì—°ì‚°ì„ ìµœì í™”í•˜ëŠ” í•µì‹¬ ìš”ì†Œ |
| `flash_mla_with_kvcache` | FlashMLAì˜ í•µì‹¬ ì—°ì‚° ìˆ˜í–‰ | ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” í•µì‹¬ ë¡œì§ |
| `scaled_dot_product_attention` | ê¸°ì¡´ Attentionê³¼ ë¹„êµ | FlashMLA ìµœì í™” íš¨ê³¼ë¥¼ ì´í•´í•˜ëŠ” ë° í•„ìˆ˜ |

ê²°êµ­, `flash_mla_with_kvcache`ë‘ `get_mla_metadata` ì´ê²Œ ì œì¼ ì¤‘ìš”í•˜ë‹¤.  

## ë§ˆë¬´ë¦¬  
ë‹¤ìŒì—ëŠ”, ìœ„ ì € ë‘ í•¨ìˆ˜ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì.  