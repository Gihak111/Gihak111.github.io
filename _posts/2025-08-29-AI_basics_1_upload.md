---
layout: single
title:  "AI 입문 1편: 토큰화에서 소프트맥스까지"
categories: "AI"
tag: "Explanation"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# AI 입문 시리즈
이번에, AI 아키텍쳐를 다루면서, 나도 까먹고 있던 개념들을 많이 되새겼었다.  
이왕 이렇게 된거, AI 기본만 빼서 한번에 이해를 위해 다시한번 더 정리해 볼까 한다.  

이번 1편에서는 **토큰화(Tokenization) → 임베딩(Embedding) → 벡터와 행렬 → 소프트맥스(Softmax) → 크로스 엔트로피(Cross-Entropy)**까지를 다룬다.  

---

## 0. 이번 편의 핵심

* **토크나이징** : 사람의 언어를 모델이 이해할 수 있게 해 주는 과정이다
* 이를 **임베딩 하여** 백터로 변환한다
* **소프트맥스(Softmax)**는 모든 라벨에 대한 확률값의 합을 1로 한다.  
* **크로스 엔트로피(Cross-Entropy)**를 통해 예측과 정답 사이를 측정한다.  

---

## 1. 토큰화 (Tokenization)

자연어는 모델이 곧바로 이해할 수 없다.  
예를 들어 문장 **"나는 밥을 먹었다"** 가 있다고 하자.  
모델에게 이 문장은 그저 문자열일 뿐이다.  

그래서 먼저 **토큰화(tokenization)** 라는 과정을 거친다.  
이는 문장을 더 작은 단위(토큰)로 쪼개는 작업이다.  

예시:  
```

"나는 밥을 먹었다"
→ \["나는", "밥", "을", "먹", "었다"]

```

하지만 실제 모델에서는 **단어 단위**보다는 **서브워드 단위**를 많이 쓴다.  
T5에서는 **SentencePiece**라는 알고리즘을 사용한다.  

예:  
```

"unbelievable"
→ \["un", "believ", "able"]

```

즉, 자주 쓰이는 단어 조각들을 토큰 단위로 삼는 것이다.  

---

## 2. 임베딩 (Embedding)

토큰으로 쪼갰다고 해도, 모델은 여전히 그것을 **문자열**로 이해한다.  
AI 모델이 이해할 수 있는 형태는 **숫자(벡터)** 뿐이다.  

따라서 각 토큰을 **고정 차원의 숫자 벡터**로 변환해야 한다.  
이 과정을 **임베딩(embedding)** 이라 한다.  

수학적으로는 다음과 같이 표현한다.

$$
\text{임베딩}: \quad \text{token } t \mapsto \mathbf{e}_t \in \mathbb{R}^d
$$

즉, 토큰 $t$를 $d$차원 실수 벡터 $\mathbf{e}_t$로 매핑한다는 뜻이다.  
예를 들어, $d=4$라면:  

```

"밥" → \[0.12, -0.93, 0.44, 0.77]

```

---

## 3. 벡터와 행렬

이제 임베딩을 이해하려면 벡터(vector)와 행렬(matrix)의 기본 개념을 짚고 넘어가야 한다.  

- **벡터 (Vector)**: 숫자들의 일렬 배열  
  $$
  \mathbf{v} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
  $$

- **행렬 (Matrix)**: 숫자들의 직사각형 배열  
  $$
  A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix}
  $$

행렬과 벡터는 주로 **곱셈**으로 연결된다.  
예를 들어,  

$$
A \cdot \mathbf{v} =
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6
\end{bmatrix}
\begin{bmatrix}
1 \\ 2
\end{bmatrix}
=
\begin{bmatrix}
5 \\ 11 \\ 17
\end{bmatrix}
$$

즉, 입력 벡터를 행렬과 곱하면 새로운 벡터가 만들어진다.  
이것이 바로 **신경망의 기본 원리**다.  

---

## 4. 소프트맥스 (Softmax)

임베딩된 벡터들이 모델을 통과하고 나면, 마지막 단계에서 모델은 **어떤 단어가 다음에 나올 확률**을 계산해야 한다.  
여기서 쓰이는 것이 **소프트맥스 함수(softmax function)**다.  

수식은 다음과 같다.

$$
\text{softmax}(z_i) = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
$$

여기서  
- $z_i$ = i번째 단어의 점수 (logit)  
- $\exp$ = 지수 함수  

즉, 모델이 "이 단어일 것 같다"라는 점수 $z_i$를 주면,  
소프트맥스는 그것을 **확률 분포**로 바꿔준다.  

예시:  
```

logit = \[2.0, 1.0, 0.1]

softmax = \[0.65, 0.24, 0.11]

```

즉, 첫 번째 단어가 나올 확률이 65%라는 의미다.  

---

## 5. 크로스 엔트로피 (Cross-Entropy)

모델을 학습시킬 때는, "예측 확률"과 "정답" 사이의 차이를 계산해야 한다.  
이를 위한 대표적인 손실 함수가 **크로스 엔트로피(cross-entropy)**다.  

정의는 다음과 같다.

$$
H(p, q) = - \sum_{i} p(i) \log q(i)
$$

여기서  
- $p(i)$ = 실제 정답 분포  
- $q(i)$ = 모델이 예측한 확률 분포  

예를 들어 정답이 단어 A 하나뿐이라면:  
- $p = [1, 0, 0]$  
- $q = [0.7, 0.2, 0.1]$  

계산:  
$$
H(p, q) = -(1 \cdot \log 0.7 + 0 \cdot \log 0.2 + 0 \cdot \log 0.1) = -\log 0.7
$$

즉, 정답 확률이 높을수록 손실이 작아진다.  

---

## 이번 편 요약

- **토큰화**: 문장을 토큰 단위로 나눈다.  
- **임베딩**: 토큰을 숫자 벡터로 바꾼다.  
- **벡터/행렬**: 신경망의 기본 연산 도구  
- **소프트맥스**: 점수를 확률로 변환한다.  
- **크로스 엔트로피**: 예측과 정답의 차이를 측정하는 손실 함수  

---

## 결론
오늘은, 딥러닝에 기초가 되는 토큰화, 소프트맥스 등을 알아보았다.  
다음에는 미분, 적분이 어떻게 녹아들어가 dense 레이어가 만들어 지는지,  
또 어떤 중요한 레이어가 있는지 알아보는 시간을 가져볼까 한다.  
