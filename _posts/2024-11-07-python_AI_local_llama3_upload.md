---
layout: single
title:  "lama, Falcon ë¡œì»¬ ì„¤ì¹˜ ë° ì‚¬ìš©í•´ë³´ê¸°"
categories: "pynb"
tag: "code"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# llama3.1
ë©”íƒ€ì—ì„œ ë¼ë§ˆ 3.1ì„ ë¿Œë ¸ë‹¤.  
ê·¸ì € ê³ íŠ¸. ê·¸ë˜ì„œ ì´ë²ˆì—ëŠ” lama3ë¥¼ ë¡œì»¬ì— ë‹¤ìš´ ë°›ê³ , í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•´ ìì‹ ë§Œì˜ AIë¥¼ ë§Œë“¤ì–´ ë³´ì.  


# ëª¨ë¸ ì¢…ë¥˜ ë° ê·œê²©
ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ë“¤ì´ ìˆë‹¤.  

## 1. 8B  
ê¶Œì¥ ì‚¬ì–‘  
- RAM: 32GB  
- VRAM: 24G  
- storage: 5G  

## 2. 70B  
ê¶Œì¥ ì‚¬ì–‘  
- RAM: 120GB  
- VRAM: 80G  
- storage: 5G  

## 3. 45B  
ê¶Œì¥ ì‚¬ì–‘  
- RAM: 500GB  
- VRAM: 400G  
- storage: 5G  

ìœ„ ì‚¬ì–‘ë³´ë‹¤ ì¡°ê¸ˆì€ ë‚®ì•„ë„ ëŒì•„ê°€ê¸°ëŠ” í•œë‹¤. 8B ëª¨ë¸ì€ ì›¬ë§Œí•˜ë©´ ì˜ ëŒë¦´ ìˆ˜ ìˆë‹¤.  
ë”°ë¼ì„œ, 8Bë¡œ ì§„í–‰í•˜ë„ë¡ í•˜ì.  

## 1. ì„¤ì¹˜ ë°©ë²•
Ollamaì—ì„œ ë‹¤ìš´ë°›ì.  
[https://ollama.com/download](https://ollama.com/download)  
ìœ„ ë§í¬ì— ë“¤ì–´ê°€ì„œ, Ollamaë¥¼ ë‹¤ìš´ë°›ê³ , ì„¤ì¹˜í•˜ì.  

ìœ„ì—ì„œ ë‹¤ìš´ë°›ì€ íŒŒì¼ì´ ì‹¤í–‰ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì²˜ìŒ í™”ë©´ì—ì„œ ë°‘ì— ì‘ê²Œ ìˆëŠ” Lama3.1ì„ ëˆ„ë¥´ê³ , ì›í•˜ëŠ” ëª¨ë¸ì„ ëˆŒëŸ¬ 8b, 4.7GBë¥¼ ì„ íƒí•´ ëª…ë ¹ì–´ë¥¼ ë°›ì.  
[ì•„ë‹ˆë©´, ì´ ë§í¬ë¡œ ë“¤ì–´ê°€ë„ ìˆë‹¤.](https://ollama.com/library/llama3.1:8b)
```bash
ollama run llama3.1:8b
```
ì´ëŸ°ì‹ì˜ ë¬¸êµ¬ê°€ ìˆëŠ”ë°, ì´ë¥¼ terminalì— ì§‘ì–´ë„£ëŠ” ê²ƒìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ë¼ë§ˆ1ì„ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆë‹¤.  
ì‘ì€ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì—, ê¸ˆë°© ë‹¤ìš´ë¡œë“œ ëœë‹¤.  
ì´ ë°©ë²•ìœ¼ë¡œ í•´ë„, ì˜ ë‹¤ìš´ë°›ì•„ì§€ë©° ì„¤ì¹˜ëœë‹¤.  
ìŠ¤í…Œì´ë¸” ë””í“¨ì „ë„ ì•Œì•„ë³´ì.  
ì•„ì£¼ ë‚˜ì´ìŠ¤ í•˜ë‹¤.  

ë” ìµœê·¼ ëª¨ë¸ì¸ ë¼ë§ˆ 3.2ë„ ìˆìœ¼ë‹ˆ ì´ê±¸ ë‹¤ìš´ë°›ì•„ë„ ëœë‹¤.  
3.2ê°€ ì„±ëŠ¥, íš¨ìœ¨ ì „ë¶€ë‹¤ ì•ì„ ë‹¤. ìµœì í™”ì— ê²½ëŸ‰í™” ê°€ì§€ ë˜ì–´ìˆìœ¼ë‹ˆ, 3.2 ì‚¬ìš©ì„ ì ê·¹ ê¶Œì¥í•œë‹¤.  

---

ë‹¤ìš´ë°›ì€ ë¼ë§ˆë¥¼ ë³´ë©´, í—¤ì‹œ í•¨ìˆ˜ SHA 256ìœ¼ë¡œ ì•”í˜¸í™” ë˜ì–´ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.  
ë”°ë¼ì„œ ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ í•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì—,  
ì´í›„ë¶€í„°ëŠ”  Falcon-7B ëª¨ë¸ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.  
## 2. í•œêµ­ì–´ íŒ¨ì¹˜í•˜ê¸°
ì¼ë‹¨, í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë”¥ëŸ¬ë‹ ì‹œì¼œë³´ì.  
ê³µê°œë˜ì–´ ìˆëŠ” í•œêµ­ì–´ ë°ì´í„° ì…‹ì—ëŠ” ëª¨ë‘ì˜ ë§ë­‰ì¹˜, AI Hubì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹, Kaist ë§ë­‰ì¹˜ ë“±ì´ ìˆë‹¤.  
ì´ì— ì•ì„œ, ìš°ì„  í•œë²ˆ ì‚¬ìš©í•´ ë³´ì.  
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
def generate_response(prompt, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # ëª¨ë¸ì´ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì„¤ì • (GPUê°€ ìˆìœ¼ë©´ GPU ì‚¬ìš©)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    inputs = {key: val.to(device) for key, val in inputs.items()}
    
    # ëª¨ë¸ì„ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±
    outputs = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # ìƒì„±ëœ ë‹µë³€ì„ ë””ì½”ë”©í•˜ì—¬ ë°˜í™˜
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# ì‹¤ì‹œê°„ ëŒ€í™”
print("ì‹¤ì‹œê°„ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ì„¸ìš”.")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["ì¢…ë£Œ", "exit", "quit"]:
        print("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    
    # ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ëª¨ë¸ ì‘ë‹µ ìƒì„±
    response = generate_response(user_input)
    print("Bot:", response)

```
ìœ„ ì½”ë“œë¡œ ì‹¤ì‹œê°„ ëŒ€í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.  
ë¬¼ë¡ , ì»´í“¨í„°ê°€ ìƒë‹¹íˆ ì¢‹ì•„ì•¼ í•œë‹¤.  

í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥´ ã„¹í†µí•´ íŒŒì¸íŠœë‹ í•  ìˆ˜ ìˆë‹¤.  
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (Falcon-7B ëª¨ë¸ ê²½ë¡œ)
model_name = "tiiuae/falcon-7b"
logger.info(f"Loading tokenizer and model from {model_name}...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# íŒ¨ë”© í† í° ì„¤ì • (í•„ìš”ì— ë”°ë¼ eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ìƒˆë¡œìš´ í† í°ì„ ì¶”ê°€)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token  # ë˜ëŠ” ìƒˆë¡œ ì¶”ê°€í•˜ë ¤ë©´ `{'pad_token': '[PAD]'}` ì‚¬ìš©
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ (KLUE MRC íƒœìŠ¤í¬ ì‚¬ìš©)
logger.info("Loading KLUE MRC dataset...")
dataset = load_dataset("klue", "mrc")

# ë°ì´í„°ì…‹ í…ìŠ¤íŠ¸ ë³€í™˜ (ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì´ì–´ ë¶™ì¸ í˜•íƒœë¡œ íŒŒì¸íŠœë‹ìš© ë°ì´í„° ìƒì„±)
def preprocess_function(examples):
    texts = []
    for question, answers in zip(examples["question"], examples["answers"]):
        # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        question_text = question if isinstance(question, str) else " ".join(question)
        # ì²« ë²ˆì§¸ ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        answer_text = answers["text"][0] if answers and "text" in answers and answers["text"] else ""
        
        # ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ í…ìŠ¤íŠ¸ ìƒì„±
        if question_text.strip() and answer_text.strip():
            combined_text = question_text + " " + answer_text
            texts.append(combined_text)
    return {"text": texts}

logger.info("Preprocessing dataset...")
# ë°ì´í„°ì…‹ì— ëŒ€í•´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ë° ë¹ˆ ìƒ˜í”Œ ì œê±°
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)

# í…ìŠ¤íŠ¸ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ìƒ˜í”Œ í•„í„°ë§ (í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° ì œê±°)
logger.info("Filtering invalid samples...")
tokenized_dataset = tokenized_dataset.filter(lambda example: len(example["text"].strip()) > 10)

# í•„í„°ë§ í›„ ë°ì´í„°ì…‹ì˜ í¬ê¸° í™•ì¸
for split in ["train", "validation"]:
    split_size = len(tokenized_dataset[split])
    logger.info(f"'{split}' split size after filtering: {split_size}")

    if split_size == 0:
        raise ValueError(f"'{split}' split has no valid samples after filtering.")

# í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding='max_length', max_length=512)

logger.info("Tokenizing dataset...")
tokenized_dataset = tokenized_dataset.map(tokenize_function, batched=True)

# í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

# DataCollator ì„¤ì • (ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ì½œë ˆì´í„°)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir=r"C:\Falcon_AI",
    overwrite_output_dir=True,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    save_steps=500,  # ë” ìì£¼ ì €ì¥í•˜ë„ë¡ ì¡°ì •
    save_total_limit=2,
    learning_rate=5e-5,
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="steps",
    logging_dir=r"C:\Falcon_AI\logs",
)

logger.info("Initializing Trainer...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# ëª¨ë¸ í•™ìŠµ
logger.info("Starting training...")
trainer.train()

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
logger.info("Saving model and tokenizer...")
model.save_pretrained(r"C:\Falcon_AI")
tokenizer.save_pretrained(r"C:\Falcon_AI")
logger.info("Training and saving completed successfully.")

```
ìœ„ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë”¥ëŸ¬ë‹ ì‹œì¼œ ëª¨ë¸ì„ ì—…ê·¸ë ˆì´ë“œ í•  ìˆ˜ ìˆë‹¤.  
ì´ê±¸ ê³„ì† ë°˜ë³µí•˜ëŠ” ê²ƒìœ¼ë¡œ ë¡œì»¬ì— ìˆëŠ” ë‚´ AIëŠ” ì ì  ê°•í•´ì§„ë‹¤.  

## 3. ë‚´ê°€ ì›í•˜ëŠ” ëŒ€ë¡œ
ì˜ˆë¥¼ë“¤ì–´, ë‚˜ì˜ AIê°€ ì´ëŸ° ë§íˆ¬ë¡œ ëŒ€ë‹µì„ í•´ ì¤¬ìœ¼ë©´ ì¢‹ê² ë‹¤ ì‹¶ì€ê²Œ ìˆìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì´ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.  

1. ë°ì´í„° ìˆ˜ì§‘
ì›í•˜ëŠ” ìºë¦­í„°ì˜ ì–´íˆ¬ì™€ ì„±ê²©ì„ ë°˜ì˜í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•œë‹¤.  
ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„, ë‹¨ì–´, ë¬´ì¥ ìŠ¤íƒ€ì¼ì„ í¬í•¨í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“¤ì.  
ì´ê±¸ jsonì´ë‚˜, txtë¡œ ë§Œë“¤ì–´ í™œìš©í•˜ë©´ ë„ë‹ˆë‹¤.  

txt ì˜ˆì‹œ
```bash
ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”! ë‹¹ì‹ ì€ ì–´ë•Œìš”? í˜¹ì‹œ ì»¤í”¼ í•œ ì” ê°™ì´ ë§ˆì‹œê³  ì‹¶ìœ¼ì„¸ìš”?
ì €ëŠ” ì™„ë²½í•˜ì§€ ì•Šì§€ë§Œ, ì¡°ê¸ˆì”© ë‚˜ì•„ì§€ê³  ìˆì–´ìš”!
ê¸°ë¶„ ì¢‹ì€ ë‚ ì—ëŠ” ì„¸ìƒì´ ë‹¤ ì˜ ë˜ëŠ” ê²ƒ ê°™ì£ ? ê³„ì† ì›ƒì–´ìš”, ì €ëŠ” í•­ìƒ ë‹¹ì‹ ì„ ì‘ì›í•´ìš”! ğŸ˜
ì˜¤ëŠ˜ì€ ë­í•˜ê³  ì§€ë‚´ìš”?

```

json ì˜ˆì‹œ
```json
{
  "conversations": [
    {
      "user": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œìš”?",
      "character": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”! ë‹¹ì‹ ì€ ì–´ë•Œìš”? í˜¹ì‹œ ì»¤í”¼ í•œ ì” ê°™ì´ ë§ˆì‹œê³  ì‹¶ìœ¼ì„¸ìš”?"
    },
    {
      "user": "ì € ì˜¤ëŠ˜ ì™„ì „íˆ ë§í–ˆì–´ìš”!",
      "character": "ì–´ë¨¸, ë­” ì¼ì´ ìˆì—ˆë‚˜ìš”? ì•„ë§ˆë„ ê·¸ëƒ¥... ê·¸ëŸ´ ìˆ˜ë„ ìˆì–´ìš”! ëˆ„êµ¬ë‚˜ ì‹¤ìˆ˜ëŠ” í•˜ë‹ˆê¹Œìš”. ì´ì œ ê·¸ ì‹¤ìˆ˜ë„ ì›ƒì„ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì¶”ì–µì´ ë  ê±°ì˜ˆìš”! ğŸ˜„"
    },
    {
      "user": "ì´ë ‡ê²Œ ë³µì¡í•œ ë¬¸ì œëŠ” ì²˜ìŒì´ì—ìš”.",
      "character": "ë³µì¡í•œ ë¬¸ì œë¼ë‹ˆ, ê±±ì • ë§ˆì„¸ìš”! ì‚¬ì‹¤, ì¸ìƒë„ ë³µì¡í•˜ê¸´ í•˜ì£ ! ğŸ˜… ë‹¤ í•´ê²°ë  ê±°ì˜ˆìš”, ì œê°€ ë„ì™€ì¤„ê²Œìš”!"
    }
  ]
}

```

ì €ê¸°ì— ìì£¼ ì‚¬ìš©í•˜ëŠ” í‚¤ì›Œë“œ ê°™ì€ê±¸ ì§‘ì–´ë„£ì–´ì„œ ì„±ê²©ì„ ë°˜ì˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤.  
ìœ„ì²˜ëŸ¼ í•˜ë©´ ëŒ€í™”ë‚´ìš©, ì–´íˆ¬, íŠ¹ì§•ì ì¸ í‘œí˜•ì´ í¬í•¨ë˜ê²Œë” í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.  

2. ë°ì´í„° ì¶”ê°€ í•™ìŠµ  
ì´ì œ, íŒŒì¸íŠœë‹ í•˜ì.  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (Falcon-7B)
model_name = "tiiuae/falcon-7b"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ìºë¦­í„° ëŒ€í™” ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
dataset = load_dataset("text", data_files="character_data.txt")  # ìºë¦­í„° ë°ì´í„° ê²½ë¡œ ì§€ì •

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•„ìš”ì‹œ ìˆ˜ì • ê°€ëŠ¥)
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì ìš©
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# íŠ¸ë ˆì´ë‹ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./falcon_character_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    learning_rate=5e-5,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    tokenizer=tokenizer,
)

# ëª¨ë¸ í•™ìŠµ
trainer.train()

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./falcon_character_finetuned")
tokenizer.save_pretrained("./falcon_character_finetuned")

```  
ìœ„ ë°©ë²•ìœ¼ë¡œ ì¶”ê°€ì ì¸í•™ìŠµì„ ì‹œí–‰í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

3. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ë§  
í›„ì— ì‚¬ìš©í•  ë•Œ, ì§ˆë¬¸ì‹œ ì•½ê°„ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì—¬ ìºë¦­í„°ì˜ ëŠë‚Œë„ ì‚´ë¦´ ìˆ˜ ìˆë‹¤.  
ì˜ˆë¥¼ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ ìˆë‹¤.  

```python
prompt = "ì£¼ì¸ê³µì²˜ëŸ¼ ë§í•´ì¤˜: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë–¤ê°€ìš”?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
output = model.generate(**inputs, max_length=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))

```  

ì „ì²´ ì½”ë“œë¡œ ë³´ë©´,  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Falcon-7B ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "tiiuae/falcon-7b"  # Falcon-7B ëª¨ë¸ ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt = "ì£¼ì¸ê³µì²˜ëŸ¼ ë§í•´ì¤˜: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë–¤ê°€ìš”?"

# ì…ë ¥ í† í°í™”
inputs = tokenizer(prompt, return_tensors="pt").to(device)

# ëª¨ë¸ ì‹¤í–‰ ë° ì¶œë ¥ ìƒì„±
output = model.generate(**inputs, max_length=50)

# ì¶œë ¥ ë””ì½”ë”©
print(tokenizer.decode(output[0], skip_special_tokens=True))

```
ì´ëŸ°ì‹ì´ë‹¤.  
íŒì„ ì£¼ìë©´, ìºë¦­í„°ê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” íŠ¹ì • í‘œí˜„ì´ë‚˜ ë°˜ì‘ì„ ë°ì´í„°ì…‹ì— ë°˜ë³µì ìœ¼ë¡œ í¬í•¨ì‹œí‚¤ë©´ ëª¨ë¸ì´ ì´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ê²Œëœë‹¤.  
ê°ì •ì„ ì‚´ë¦¬ê³  ì‹¶ìœ¼ë©´ í•´ë‹¹ ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì¥ íŒ¨í„´ì„ ë‹¤ì–‘í•˜ê²Œ í¬í•¨í•´ ëª¨ë¸ì´ ì´ë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ë©´ ëœë‹¤.  

## 4. ì½”ë”© ì§€ì‹ ì£¼ì…í•˜ê¸°
ìœ„ì—ì„œ í•œ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, íŒŒì¸ íŠœë‹ì„ í†µí•´ì„œ ë„ˆê°€ ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ê±”ì† ì¶”ê°€í•´ ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤.  
ì´ë²ˆì—” ì½”ë”© ì§€ì‹ì„ ì£¼ì…í•´ ë³´ì.  
ì´ë¯¸ ì½”ë”© ê´€ë ¨ ë°ì´í„° ì…‹ì€ ì—„ì²­ë‚˜ê²Œ ë§ì´ ìˆë‹¤. ì¢…ë¥˜ë¥¼ ì•Œì•„ë³´ìë©´
- CodeSearchNet
    ê¹ƒí—ˆë¸Œì—ì„œ ìˆ˜ì§‘ëœ ë‚´ìš©ì´ë©°, Python, Java, JavaScript, PHP, Ruby, Goë“±ì„ í¬í•¨í•œë‹¤.  
    ì‚¬ìš©ì€, 
    ```python
    from datasets import load_dataset

    dataset = load_dataset("code_search_net", "python")

    ```
    ì´ë ‡ê²Œ í•  ìˆ˜ ìˆë‹¤.  

- Github Repositories
    ê¹ƒí—ˆë¸Œì˜ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì˜ ë‚´ìš©ì„ í¬í•¨í•œë‹¤.  
    Hugging Faceì™€ BigCode í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë°ì´í„°ì…‹ì„ í™•ë³´í•  ìˆ˜ ìˆë‹¤.  

- CodeParrot
    ì´ê±´ íŒŒì´ì¬ ë‹¨ì¼ì´ë©° íŒŒì´ì¬ ì½”ë“œ ìƒì„±ì— ìµœì í™” ì‹œí‚¬ ìˆ˜ ìˆë‹¤.  
    ```python
    dataset = load_dataset("codeparrot/codeparrot-clean")
    ```

- Python Dataset by Hugging Face
    í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ, ì¦‰, ai ìƒì„±ì— íŠ¹í™”ëœ ë‚´ìš©ë“¤ì´ ë‹´ê²¨ì ¸ ìˆë‹¤.  
    ì½”ë“œ ì˜ˆì œì™€ ì£¼ì„, ë¬¸ì„œí™”ëœ ì½”ë“œ ë“±ì´ í¬í•¨ë˜ì–´ ìˆì–´ ìì—°ìŠ¤ëŸ½ê³  ìœ ìš©í•œ ì½”ë”© í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë° ìœ ë¦¬í•˜ë‹¤.  
    ```python
    dataset = load_dataset("codeparrot/github-code")
    ```

ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì…‹ë“¤ë¡œ êµ¬ì„±í•˜ì—¬ ë§Œë“¤ì–´ ë³´ì.  
```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install transformers datasets torch

from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset, concatenate_datasets
import torch

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (Falcon-7B ëª¨ë¸)
model_name = "tiiuae/falcon-7b"  # Falcon-7B ëª¨ë¸ ê²½ë¡œ
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Step 1: ë‹¤ì–‘í•œ ì½”ë”© ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
print("Loading datasets...")

# CodeSearchNet ë°ì´í„°ì…‹ (Python ì½”ë“œ)
codesearchnet = load_dataset("code_search_net", "python", split="train[:5%]")

# CodeParrot ë°ì´í„°ì…‹ (Python ì½”ë“œ)
codeparrot = load_dataset("codeparrot/codeparrot-clean", split="train[:5%]")

# APPS ë°ì´í„°ì…‹ (Python ì½”ë“œ ë¬¸ì œì™€ ì†”ë£¨ì…˜)
apps = load_dataset("HuggingFaceH4/APPS", split="train[:5%]")

# ë°ì´í„°ì…‹ ë³‘í•©
dataset = concatenate_datasets([codesearchnet, codeparrot, apps])

# Step 2: ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def preprocess_function(examples):
    # ì½”ë“œê°€ ìˆëŠ” ì—´ì„ ì„ íƒ (datasetë§ˆë‹¤ ì—´ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    code_samples = examples["code"] if "code" in examples else examples["text"]
    inputs = tokenizer(code_samples, padding="max_length", truncation=True, max_length=128)
    inputs["labels"] = inputs["input_ids"].copy()  # ì–¸ì–´ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ labels ì„¤ì •
    return inputs

# ì „ì²´ ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ ì ìš©
print("Tokenizing dataset...")
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)

# Step 3: íŠ¸ë ˆì´ë‹ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./falcon_code_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    learning_rate=5e-5,
    logging_dir='./logs',
    logging_steps=10,
)

# Step 4: Trainer ì´ˆê¸°í™” ë° í•™ìŠµ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# í•™ìŠµ ì‹œì‘
print("Starting training...")
trainer.train()

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./falcon_code_finetuned")
tokenizer.save_pretrained("./falcon_code_finetuned")
print("Training completed and model saved.")

```
ìœ„ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ìì‹ ë§Œì˜ AIë¥¼ ê³„ì†í•´ì„œ ê°•í™”í•  ìˆ˜ ìˆë‹¤.