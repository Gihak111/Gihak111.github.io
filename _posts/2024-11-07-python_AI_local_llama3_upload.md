---
layout: single
title:  "lama ë¡œì»¬ ì„¤ì¹˜ ë° ì‚¬ìš©í•´ë³´ê¸°"
categories: "pynb"
tag: "code"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# llama3.1
ë©”íƒ€ì—ì„œ ë¼ë§ˆ 3.1ì„ ë¿Œë ¸ë‹¤.  
ê·¸ì € ê³ íŠ¸. ê·¸ë˜ì„œ ì´ë²ˆì—ëŠ” lama3ë¥¼ ë¡œì»¬ì— ë‹¤ìš´ ë°›ê³ , í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•´ ìì‹ ë§Œì˜ AIë¥¼ ë§Œë“¤ì–´ ë³´ì.  


# ëª¨ë¸ ì¢…ë¥˜ ë° ê·œê²©
ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ë“¤ì´ ìˆë‹¤.  

## 1. 8B  
ê¶Œì¥ ì‚¬ì–‘  
- RAM: 32GB  
- VRAM: 24G  
- storage: 5G  

## 2. 70B  
ê¶Œì¥ ì‚¬ì–‘  
- RAM: 120GB  
- VRAM: 80G  
- storage: 5G  

## 3. 45B  
ê¶Œì¥ ì‚¬ì–‘  
- RAM: 500GB  
- VRAM: 400G  
- storage: 5G  

ìœ„ ì‚¬ì–‘ë³´ë‹¤ ì¡°ê¸ˆì€ ë‚®ì•„ë„ ëŒì•„ê°€ê¸°ëŠ” í•œë‹¤. 8B ëª¨ë¸ì€ ì›¬ë§Œí•˜ë©´ ì˜ ëŒë¦´ ìˆ˜ ìˆë‹¤.  
ë”°ë¼ì„œ, 8Bë¡œ ì§„í–‰í•˜ë„ë¡ í•˜ì.  

## 1. ì„¤ì¹˜ ë°©ë²•
Ollamaì—ì„œ ë‹¤ìš´ë°›ì.  
[https://ollama.com/download](https://ollama.com/download)  
ìœ„ ë§í¬ì— ë“¤ì–´ê°€ì„œ, Ollamaë¥¼ ë‹¤ìš´ë°›ê³ , ì„¤ì¹˜í•˜ì.  

ìœ„ì—ì„œ ë‹¤ìš´ë°›ì€ íŒŒì¼ì´ ì‹¤í–‰ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì²˜ìŒ í™”ë©´ì—ì„œ ë°‘ì— ì‘ê²Œ ìˆëŠ” Lama3.1ì„ ëˆ„ë¥´ê³ , ì›í•˜ëŠ” ëª¨ë¸ì„ ëˆŒëŸ¬ 8b, 4.7GBë¥¼ ì„ íƒí•´ ëª…ë ¹ì–´ë¥¼ ë°›ì.  
[ì•„ë‹ˆë©´, ì´ ë§í¬ë¡œ ë“¤ì–´ê°€ë„ ìˆë‹¤.](https://ollama.com/library/llama3.1:8b)
```bash
ollama run llama3.1:8b
```
ì´ëŸ°ì‹ì˜ ë¬¸êµ¬ê°€ ìˆëŠ”ë°, ì´ë¥¼ terminalì— ì§‘ì–´ë„£ëŠ” ê²ƒìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ë¼ë§ˆ1ì„ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆë‹¤.  
ì‘ì€ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì—, ê¸ˆë°© ë‹¤ìš´ë¡œë“œ ëœë‹¤.  
ì´ ë°©ë²•ìœ¼ë¡œ í•´ë„, ì˜ ë‹¤ìš´ë°›ì•„ì§€ë©° ì„¤ì¹˜ëœë‹¤.  
ìŠ¤í…Œì´ë¸” ë””í“¨ì „ë„ ì•Œì•„ë³´ì.  
ì•„ì£¼ ë‚˜ì´ìŠ¤ í•˜ë‹¤.  

ë” ìµœê·¼ ëª¨ë¸ì¸ ë¼ë§ˆ 3.2ë„ ìˆìœ¼ë‹ˆ ì´ê±¸ ë‹¤ìš´ë°›ì•„ë„ ëœë‹¤.  
3.2ê°€ ì„±ëŠ¥, íš¨ìœ¨ ì „ë¶€ë‹¤ ì•ì„ ë‹¤. ìµœì í™”ì— ê²½ëŸ‰í™” ê°€ì§€ ë˜ì–´ìˆìœ¼ë‹ˆ, 3.2 ì‚¬ìš©ì„ ì ê·¹ ê¶Œì¥í•œë‹¤.  

## 2. í•œêµ­ì–´ íŒ¨ì¹˜í•˜ê¸°
ì¼ë‹¨, í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë”¥ëŸ¬ë‹ ì‹œì¼œë³´ì.  
ê³µê°œë˜ì–´ ìˆëŠ” í•œêµ­ì–´ ë°ì´í„° ì…‹ì—ëŠ” ëª¨ë‘ì˜ ë§ë­‰ì¹˜, AI Hubì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹, Kaist ë§ë­‰ì¹˜ ë“±ì´ ìˆë‹¤.  

í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥´ ã„¹í†µí•´ íŒŒì¸íŠœë‹ í•  ìˆ˜ ìˆë‹¤.  
```python
from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "path_to_downloaded_llama_model"
tokenizer = LlamaTokenizer.from_pretrained(model_name)
model = LlamaForCausalLM.from_pretrained(model_name)

# ë°ì´í„°ì…‹ ë¡œë“œ (í•œêµ­ì–´ ë°ì´í„°ì…‹ ê²½ë¡œ ì§€ì •)
dataset = load_dataset("text", data_files="korean_data.txt")

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./llama_korean_finetuned",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    save_steps=10_000,
    save_total_limit=2,
    learning_rate=5e-5,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    tokenizer=tokenizer,
)

# ëª¨ë¸ í•™ìŠµ
trainer.train()

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./llama_korean_finetuned")
tokenizer.save_pretrained("./llama_korean_finetuned")

```
ìœ„ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë”¥ëŸ¬ë‹ ì‹œì¼œ ëª¨ë¸ì„ ì—…ê·¸ë ˆì´ë“œ í•  ìˆ˜ ìˆë‹¤.  
ì´ê±¸ ê³„ì† ë°˜ë³µí•˜ëŠ” ê²ƒìœ¼ë¡œ ë¡œì»¬ì— ìˆëŠ” ë‚´ AIëŠ” ì ì  ê°•í•´ì§„ë‹¤.  

## 3. ë‚´ê°€ ì›í•˜ëŠ” ëŒ€ë¡œ
ì˜ˆë¥¼ë“¤ì–´, ë‚˜ì˜ AIê°€ ì´ëŸ° ë§íˆ¬ë¡œ ëŒ€ë‹µì„ í•´ ì¤¬ìœ¼ë©´ ì¢‹ê² ë‹¤ ì‹¶ì€ê²Œ ìˆìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì´ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.  

1. ë°ì´í„° ìˆ˜ì§‘
ì›í•˜ëŠ” ìºë¦­í„°ì˜ ì–´íˆ¬ì™€ ì„±ê²©ì„ ë°˜ì˜í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•œë‹¤.  
ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„, ë‹¨ì–´, ë¬´ì¥ ìŠ¤íƒ€ì¼ì„ í¬í•¨í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“¤ì.  
ì´ê±¸ jsonì´ë‚˜, txtë¡œ ë§Œë“¤ì–´ í™œìš©í•˜ë©´ ë„ë‹ˆë‹¤.  

txt ì˜ˆì‹œ
```bash
ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”! ë‹¹ì‹ ì€ ì–´ë•Œìš”? í˜¹ì‹œ ì»¤í”¼ í•œ ì” ê°™ì´ ë§ˆì‹œê³  ì‹¶ìœ¼ì„¸ìš”?
ì €ëŠ” ì™„ë²½í•˜ì§€ ì•Šì§€ë§Œ, ì¡°ê¸ˆì”© ë‚˜ì•„ì§€ê³  ìˆì–´ìš”!
ê¸°ë¶„ ì¢‹ì€ ë‚ ì—ëŠ” ì„¸ìƒì´ ë‹¤ ì˜ ë˜ëŠ” ê²ƒ ê°™ì£ ? ê³„ì† ì›ƒì–´ìš”, ì €ëŠ” í•­ìƒ ë‹¹ì‹ ì„ ì‘ì›í•´ìš”! ğŸ˜
ì˜¤ëŠ˜ì€ ë­í•˜ê³  ì§€ë‚´ìš”?

```

json ì˜ˆì‹œ
```json
{
  "conversations": [
    {
      "user": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œìš”?",
      "character": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”! ë‹¹ì‹ ì€ ì–´ë•Œìš”? í˜¹ì‹œ ì»¤í”¼ í•œ ì” ê°™ì´ ë§ˆì‹œê³  ì‹¶ìœ¼ì„¸ìš”?"
    },
    {
      "user": "ì € ì˜¤ëŠ˜ ì™„ì „íˆ ë§í–ˆì–´ìš”!",
      "character": "ì–´ë¨¸, ë­” ì¼ì´ ìˆì—ˆë‚˜ìš”? ì•„ë§ˆë„ ê·¸ëƒ¥... ê·¸ëŸ´ ìˆ˜ë„ ìˆì–´ìš”! ëˆ„êµ¬ë‚˜ ì‹¤ìˆ˜ëŠ” í•˜ë‹ˆê¹Œìš”. ì´ì œ ê·¸ ì‹¤ìˆ˜ë„ ì›ƒì„ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì¶”ì–µì´ ë  ê±°ì˜ˆìš”! ğŸ˜„"
    },
    {
      "user": "ì´ë ‡ê²Œ ë³µì¡í•œ ë¬¸ì œëŠ” ì²˜ìŒì´ì—ìš”.",
      "character": "ë³µì¡í•œ ë¬¸ì œë¼ë‹ˆ, ê±±ì • ë§ˆì„¸ìš”! ì‚¬ì‹¤, ì¸ìƒë„ ë³µì¡í•˜ê¸´ í•˜ì£ ! ğŸ˜… ë‹¤ í•´ê²°ë  ê±°ì˜ˆìš”, ì œê°€ ë„ì™€ì¤„ê²Œìš”!"
    }
  ]
}

```

ì €ê¸°ì— ìì£¼ ì‚¬ìš©í•˜ëŠ” í‚¤ì›Œë“œ ê°™ì€ê±¸ ì§‘ì–´ë„£ì–´ì„œ ì„±ê²©ì„ ë°˜ì˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤.  
ìœ„ì²˜ëŸ¼ í•˜ë©´ ëŒ€í™”ë‚´ìš©, ì–´íˆ¬, íŠ¹ì§•ì ì¸ í‘œí˜•ì´ í¬í•¨ë˜ê²Œë” í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.  

2. ë°ì´í„° ì¶”ê°€ í•™ìŠµ  
ì´ì œ, íŒŒì¸íŠœë‹ í•˜ì.  

```python
from transformers import Trainer, TrainingArguments

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ì´ì „ì— ì €ì¥í•œ í•œêµ­ì–´ ëª¨ë¸ ë˜ëŠ” ì›í•˜ëŠ” ëª¨ë¸ ì‚¬ìš©)
model_name = "./llama_korean_finetuned"
model = LlamaForCausalLM.from_pretrained(model_name)
tokenizer = LlamaTokenizer.from_pretrained(model_name)

# ìºë¦­í„° ëŒ€í™” ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
dataset = load_dataset("text", data_files="character_data.txt")  # ìºë¦­í„° ë°ì´í„° ê²½ë¡œ ì§€ì •

# íŠ¸ë ˆì´ë‹ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./llama_character_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    learning_rate=5e-5,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    tokenizer=tokenizer,
)

# ëª¨ë¸ í•™ìŠµ
trainer.train()

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./llama_character_finetuned")
tokenizer.save_pretrained("./llama_character_finetuned")

```  
ìœ„ ë°©ë²•ìœ¼ë¡œ ì¶”ê°€ì ì¸í•™ìŠµì„ ì‹œí–‰í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.  

3. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ë§  
í›„ì— ì‚¬ìš©í•  ë•Œ, ì§ˆë¬¸ì‹œ ì•½ê°„ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì—¬ ìºë¦­í„°ì˜ ëŠë‚Œë„ ì‚´ë¦´ ìˆ˜ ìˆë‹¤.  
ì˜ˆë¥¼ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ ìˆë‹¤.  

```python
prompt = "ì£¼ì¸ê³µì²˜ëŸ¼ ë§í•´ì¤˜: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë–¤ê°€ìš”?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
output = model.generate(**inputs, max_length=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))

```  

ì „ì²´ ì½”ë“œë¡œ ë³´ë©´,  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "meta-llama/Llama-3.2-3b"  # ì˜ˆì‹œ: Llama 3.2 3B ëª¨ë¸
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt = "ì£¼ì¸ê³µì²˜ëŸ¼ ë§í•´ì¤˜: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë–¤ê°€ìš”?"

# ì…ë ¥ í† í°í™”
inputs = tokenizer(prompt, return_tensors="pt").to(device)

# ëª¨ë¸ ì‹¤í–‰ ë° ì¶œë ¥ ìƒì„±
output = model.generate(**inputs, max_length=50)

# ì¶œë ¥ ë””ì½”ë”©
print(tokenizer.decode(output[0], skip_special_tokens=True))

```
ì´ëŸ°ì‹ì´ë‹¤.  
íŒì„ ì£¼ìë©´, ìºë¦­í„°ê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” íŠ¹ì • í‘œí˜„ì´ë‚˜ ë°˜ì‘ì„ ë°ì´í„°ì…‹ì— ë°˜ë³µì ìœ¼ë¡œ í¬í•¨ì‹œí‚¤ë©´ ëª¨ë¸ì´ ì´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ê²Œëœë‹¤.  
ê°ì •ì„ ì‚´ë¦¬ê³  ì‹¶ìœ¼ë©´ í•´ë‹¹ ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì¥ íŒ¨í„´ì„ ë‹¤ì–‘í•˜ê²Œ í¬í•¨í•´ ëª¨ë¸ì´ ì´ë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ë©´ ëœë‹¤.  

## 4. ì½”ë”© ì§€ì‹ ì£¼ì…í•˜ê¸°
ìœ„ì—ì„œ í•œ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, íŒŒì¸ íŠœë‹ì„ í†µí•´ì„œ ë„ˆê°€ ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ê±”ì† ì¶”ê°€í•´ ë‚˜ê°ˆ ìˆ˜ ìˆë‹¤.  
ì´ë²ˆì—” ì½”ë”© ì§€ì‹ì„ ì£¼ì…í•´ ë³´ì.  
ì´ë¯¸ ì½”ë”© ê´€ë ¨ ë°ì´í„° ì…‹ì€ ì—„ì²­ë‚˜ê²Œ ë§ì´ ìˆë‹¤. ì¢…ë¥˜ë¥¼ ì•Œì•„ë³´ìë©´
- CodeSearchNet
    ê¹ƒí—ˆë¸Œì—ì„œ ìˆ˜ì§‘ëœ ë‚´ìš©ì´ë©°, Python, Java, JavaScript, PHP, Ruby, Goë“±ì„ í¬í•¨í•œë‹¤.  
    ì‚¬ìš©ì€, 
    ```python
    from datasets import load_dataset

    dataset = load_dataset("code_search_net", "python")

    ```
    ì´ë ‡ê²Œ í•  ìˆ˜ ìˆë‹¤.  

- Github Repositories
    ê¹ƒí—ˆë¸Œì˜ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì˜ ë‚´ìš©ì„ í¬í•¨í•œë‹¤.  
    Hugging Faceì™€ BigCode í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë°ì´í„°ì…‹ì„ í™•ë³´í•  ìˆ˜ ìˆë‹¤.  

- CodeParrot
    ì´ê±´ íŒŒì´ì¬ ë‹¨ì¼ì´ë©° íŒŒì´ì¬ ì½”ë“œ ìƒì„±ì— ìµœì í™” ì‹œí‚¬ ìˆ˜ ìˆë‹¤.  
    ```python
    dataset = load_dataset("codeparrot/codeparrot-clean")
    ```

- Python Dataset by Hugging Face
    í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì§‘ëœ, ì¦‰, ai ìƒì„±ì— íŠ¹í™”ëœ ë‚´ìš©ë“¤ì´ ë‹´ê²¨ì ¸ ìˆë‹¤.  
    ì½”ë“œ ì˜ˆì œì™€ ì£¼ì„, ë¬¸ì„œí™”ëœ ì½”ë“œ ë“±ì´ í¬í•¨ë˜ì–´ ìˆì–´ ìì—°ìŠ¤ëŸ½ê³  ìœ ìš©í•œ ì½”ë”© í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë° ìœ ë¦¬í•˜ë‹¤.  
    ```python
    dataset = load_dataset("codeparrot/github-code")
    ```

ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì…‹ë“¤ë¡œ êµ¬ì„±í•˜ì—¬ ë§Œë“¤ì–´ ë³´ì.  
```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install transformers datasets torch

from transformers import Trainer, TrainingArguments, LlamaForCausalLM, LlamaTokenizer
from datasets import load_dataset, concatenate_datasets
import torch

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (Llama ëª¨ë¸)
model_name = "./llama_korean_finetuned"  # ëª¨ë¸ ê²½ë¡œ
model = LlamaForCausalLM.from_pretrained(model_name)
tokenizer = LlamaTokenizer.from_pretrained(model_name)

# Step 1: ë‹¤ì–‘í•œ ì½”ë”© ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
print("Loading datasets...")

# CodeSearchNet ë°ì´í„°ì…‹ (Python ì½”ë“œ)
codesearchnet = load_dataset("code_search_net", "python", split="train[:5%]")

# CodeParrot ë°ì´í„°ì…‹ (Python ì½”ë“œ)
codeparrot = load_dataset("codeparrot/codeparrot-clean", split="train[:5%]")

# APPS ë°ì´í„°ì…‹ (Python ì½”ë“œ ë¬¸ì œì™€ ì†”ë£¨ì…˜)
apps = load_dataset("HuggingFaceH4/APPS", split="train[:5%]")

# ë°ì´í„°ì…‹ ë³‘í•©
dataset = concatenate_datasets([codesearchnet, codeparrot, apps])

# Step 2: ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def preprocess_function(examples):
    # ì½”ë“œê°€ ìˆëŠ” ì—´ì„ ì„ íƒ (datasetë§ˆë‹¤ ì—´ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    code_samples = examples["code"] if "code" in examples else examples["text"]
    inputs = tokenizer(code_samples, padding="max_length", truncation=True, max_length=128)
    inputs["labels"] = inputs["input_ids"].copy()  # ì–¸ì–´ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ labels ì„¤ì •
    return inputs

# ì „ì²´ ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ ì ìš©
print("Tokenizing dataset...")
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)

# Step 3: íŠ¸ë ˆì´ë‹ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./llama_code_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    learning_rate=5e-5,
    logging_dir='./logs',
    logging_steps=10,
)

# Step 4: Trainer ì´ˆê¸°í™” ë° í•™ìŠµ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# í•™ìŠµ ì‹œì‘
print("Starting training...")
trainer.train()

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./llama_code_finetuned")
tokenizer.save_pretrained("./llama_code_finetuned")
print("Training completed and model saved.")

```
ìœ„ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ìì‹ ë§Œì˜ AIë¥¼ ê³„ì†í•´ì„œ ê°•í™”í•  ìˆ˜ ìˆë‹¤.