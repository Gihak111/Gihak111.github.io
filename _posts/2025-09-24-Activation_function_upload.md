---
layout: single
title:  "Activation Function"
categories: "AI"
tag: "linear algebra"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## ## 활성화 함수

신경망의 각 층은 기본적으로 선형 변환(Linear Transformation)과 비선형 변환(Non-linear Transformation)의 연속이다.  

1.  **선형 변환**: 입력값에 가중치 행렬(Weight Matrix)을 곱하고 편향(bias)을 더하는 과정이다. 이는 선형대수학의 **아핀 변환(Affine Transformation)**에 해당하며, $$y = Wx + b$$로 표현된다.
2.  **비선형 변환**: 이 선형 변환 결과에 **활성화 함수(Activation Function)**를 적용하는 과정이다.

만약 활성화 함수가 없다면, 여러 겹의 층을 쌓더라도 $$y = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)$$ 와 같이 결국 하나의 거대한 선형 변환으로 축약된다.  
이는 층을 깊게 쌓는 의미를 무색하게 만든다.  
활성화 함수는 여기에 **비선형성**을 추가하여 모델이 복잡하고 다양한 패턴을 학습할 수 있도록 하는 핵심적인 역할을 한다.  


## 시그모이드(Sigmoid): 확률적 해석의 매력과 치명적 약점

초기 신경망에서 가장 각광받던 함수이다.  
그 모양이 생물학적 뉴런의 활성화(firing) 패턴과 유사하여 직관적이었기 때문이다.  

* **수식**:  
    $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
    이 함수는 모든 실수 입력 값을 0과 1 사이의 값으로 압축한다.  
    이는 출력을 **확률**처럼 해석할 수 있게 해줘서, 특히 이진 분류 문제의 마지막 출력층에 사용하기에 매우 적합했다.  

    

* **문제점: 경사 소실 (Vanishing Gradient)**  
    문제는 역전파(Backpropagation) 과정에서 발생합니다.  
    역전파는 손실 함수(Loss Function)를 각 가중치로 미분하여 얻은 기울기(Gradient)를 통해 가중치를 업데이트하는 과정이다.  
    연쇄 법칙(Chain Rule)에 따라, 앞쪽 층(입력층에 가까운 층)의 기울기는 뒤쪽 층들에서 계산된 기울기들의 곱으로 표현된다.  

    시그모이드 함수의 도함수(Derivative)를 살펴보자.  
    $$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$
    이 도함수의 최댓값은 $$x=0$$일 때 **0.25**에 불과하다.  
    입력값 $$x$$의 절댓값이 커지면 (예: $$x > 5$$또는$$x < -5$$), 출력값은 1 또는 0에 수렴하고, 기울기는 **0에 매우 가까워진다**.  

    * **선형대수학적 관점**: 역전파 과정에서 각 층의 가중치 행렬 $$W$$에 대한 기울기를 구하는 것은, 상위 층에서 넘어온 기울기(벡터 또는 행렬)에 현재 층의 활성화 함수 도함수 값들로 구성된 대각 행렬(Jacobian Matrix)을 곱하는 과정이 포함된다.  
    층이 깊어질수록, 0.25보다 작거나 0에 가까운 값들이 계속해서 곱해진다.  
    이는 마치 벡터에 0.1, 0.05 같은 작은 스칼라를 계속 곱하는 것과 같아서, 결국 기울기 벡터의 크기(magnitude)가 **지수적으로 감소하여 0에 수렴**하게 된다.  
    이 현상이 바로 **경사 소실**이다.  
    이로 인해 입력층에 가까운 층들은 가중치 업데이트가 거의 이루어지지 않아 학습이 제대로 되지 않는 문제가 발생했다.  


## ReLU(Rectified Linear Unit): 단순함의 위대한 승리

경사 소실 문제를 해결하기 위한 구원투수로 등장했으며, 현재까지도 가장 널리 사용되는 활성화 함수 중 하나이다.  

* **수식**:  
    $$ReLU(x) = max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases}$$
    매우 간단합니다. 입력이 양수이면 그대로 내보내고, 음수이면 0으로 만든다.  

* **경사 소실 해결**:  
    ReLU의 도함수는 더욱 간단하다.  
    $$ReLU'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \end{cases}$$
    (수학적으로 $$x=0$$에서 미분 불가능하지만, 공학적으로는 0 또는 1로 정의하여 사용한다.)  

    핵심은 **입력이 양수일 때 기울기가 항상 1**이라는 점입니다.  
    * **선형대수학적 관점**: 역전파 시 활성화된(입력이 양수인) 뉴런에서는 기울기에 1이 곱해진다.  
    이는 상위 층에서 전달된 기울기가 아무런 손실 없이 그대로 하위 층으로 전달된다는 의미이다.  
    덕분에 신경망이 아무리 깊어져도 기울기가 소실되지 않고 안정적으로 전달되어 깊은 네트워크의 학습이 가능해졌다.  

* **문제점: 죽은 렐루 (Dying ReLU)**
    만약 어떤 뉴런에 들어오는 입력값의 가중 합이 항상 음수가 되면, 그 뉴런의 출력은 항상 0이 된다.  
    그러면 역전파 시에도 해당 뉴런의 기울기는 항상 0이 된다.  
    한번 0이 되기 시작한 뉴런은 가중치가 더 이상 업데이트되지 않아 영원히 0만 출력하는 상태, 즉 '죽은' 상태가 될 수 있다.  
    이는 학습 과정에서 일부 뉴런들이 비활성화되어버리는(비활성 노드) 문제를 야기한다.  


## Leaky ReLU: 죽은 뉴런을 다시 살린다  

비활성 노드 문제를 해결하기 위해 ReLU를 약간 변형한 함수이다.  

* **수식**:  
    $$LeakyReLU(x) = max(\alpha x, x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases}$$
    여기서 $$\alpha$$는 0.01과 같이 매우 작은 양수(hyperparameter)이다.  
    이름처럼 음수 영역에서 약간의 기울기가 새어 나가도록 설계되었다.  

* **'죽은 렐루' 해결**:
    Leaky ReLU의 도함수를 보면 해결책이 명확해진다.  
    $$LeakyReLU'(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha & \text{if } x \le 0 \end{cases}$$
    * **선형대수학적 관점**: 이제 입력이 음수여서 뉴런의 출력이 음수가 되더라도, 기울기는 0이 아닌 작은 값 $$\alpha$$가 된다.  
    따라서 역전파 과정에서 아주 작은 기울기라도 하위 층으로 전달될 수 있다.  
    이 작은 기울기 덕분에 뉴런이 다시 활성화될 수 있는 방향으로 가중치가 업데이트될 기회를 얻게 되어, 뉴런이 완전히 죽는 현상을 방지할 수 있다.  


## GELU: Gaussian Error Linear Unit

**BERT**, **GPT** 등 현대 자연어 처리(NLP) 모델의 심장과도 같은 활성화 함수이다.  
확률론적 관점을 도입한 함수로, 입력값에 **정규 분포의 누적 분포 함수(CDF)**를 곱한 형태로, 직관적으로 '입력값이 얼마나 클지'와 '입력값이 정규분포 상에서 어느 위치에 있는지'를 모두 고려하여 출력을 결정하는, ReLU의 부드러운(smooth) 근사치로 볼 수 있다.  

* **수식**:  
    $$GELU(x) = x \cdot \Phi(x)$$    여기서$$\Phi(x)$$는 표준 정규 분포의 누적 분포 함수이다.  
    실제 구현에서는 아래의 근사식을 주로 사용한다.  
    $$GELU(x) \approx 0.5x \left( 1 + \tanh\left[\sqrt{2/\pi}(x + 0.044715x^3)\right] \right)$$
    
* **장점**:
    * **부드러운 미분값**: ReLU처럼 $$x=0$$에서 미분값이 급격히 변하지 않고 부드럽게 이어져 학습 안정성에 기여한다.
    * **탁월한 성능**: 특히 Transformer 기반의 모델들에서 매우 뛰어난 성능을 보이는 것으로 입증되었다.  
* **단점**: 수식이 복잡하여 다른 함수들에 비해 연산 비용이 높다.  

## Swish: 또는 SiLU, Sigmoid-weighted Linear Unit

Google에서 발견한 함수로, 간단하면서도 GELU와 유사한 성능을 내는 효율적인 함수입니다.  
입력값 $$x$$에 시그모이드 함수 $$\sigma(x)$$를 곱하는 **자기 게이팅(self-gating)** 구조를 가진다.  
여기서 시그모이드 함수가 '게이트' 역할을 하여 입력값 $$x$$를 얼마나 통과시킬지 조절한다.  

* **수식**:  
    $$Swish(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$
    
* **장점**:
    * **비단조성(Non-monotonicity)**: 음수 영역에서 작은 양수 값을 가졌다가 다시 0으로 감소하는 독특한 형태를 보인다. 이러한 특성이 모델의 표현력을 높여주는 것으로 알려져 있다.  
    * **부드러움과 효율성**: GELU처럼 부드러우면서도 상대적으로 계산이 간단하여, 다양한 딥러닝 모델에서 ReLU를 대체하여 성능을 향상시키는 경우가 많다.  
* **단점**: ReLU보다는 연산 비용이 높다.  


## softmax: 다중 분류를 위한 특수 활성화 함수

앞서 설명한 함수들은 주로 은닉층(hidden layer)에서 사용되지만, **소프트맥스**는 주로 **다중 클래스 분류(multi-class classification) 문제의 출력층(output layer)**에서만 사용되는 특별한 함수입니다.  
모델의 최종 출력(Logits) 벡터를 입력받아, 각 클래스에 대한 **확률 분포** 벡터로 변환한다.  
모든 출력값의 합은 항상 1이 된다.  

* **수식**: 출력 벡터 $$z = (z_1, z_2, ..., z_K)$$에 대해, i번째 클래스에 대한 확률은 다음과 같이 계산된다.  
    $$Softmax(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$
* **역할**: 예를 들어, [고양이, 개, 새]를 분류하는 모델의 최종 출력이 `[2.0, 1.0, 0.1]`로 나왔다면, 소프트맥스는 이를 `[0.7, 0.2, 0.1]`과 같이 각 클래스일 확률로 변환해 준다. 따라서 가장 확률이 높은 '고양이'를 최종 예측 결과로 선택할 수 있다.  

---

## 결론

| 함수 | **핵심 아이디어** | **주요 특징 및 사용처** |
| :--- | :--- | :--- |
| **ReLU** | 음수 값을 0으로 만든다. | **기본값(Default)**. 빠르고 대부분의 경우 성능이 준수함. |
| **Leaky ReLU** | 음수 값에 작은 기울기를 부여한다. | '죽은 렐루' 현상이 우려될 때 사용. |
| **PReLU** | 작은 기울기를 데이터로 학습한다. | Leaky ReLU보다 더 나은 성능을 원할 때 시도. |
| **ELU** | 음수 영역을 부드러운 지수 곡선으로. | 출력의 평균을 0으로 만들어 학습 안정화를 꾀할 때 사용. |
| **GELU** | 확률론적 관점의 부드러운 ReLU. | **Transformer 계열 (BERT, GPT 등) 모델의 표준.** |
| **Swish(SiLU)** | 입력에 시그모이드 게이트를 곱한다. | **최신 CNN, Vision 모델 등**에서 ReLU를 대체하여 성능 향상. |
| **Softmax** | 출력을 확률 분포로 변환한다. | **다중 클래스 분류 문제의 최종 출력층 전용.** |

어떤 활성화 함수가 모든 상황에서 최고라고 말할 수는 없다.  
- 일반적으로는 ReLU로 시작하는 것이 가장 안전하고 효율적인 선택이다.  
- 더 높은 성능을 추구한다면 Swish나 GELU와 같은 최신 함수들을 시도해 볼 수 있다.  
- 모델의 아키텍처(특히 Transformer)가 정해져 있다면 해당 아키텍처에서 표준으로 사용하는 함수(예: GELU)를 따르는 것이 좋다.  