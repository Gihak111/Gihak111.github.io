---
layout: single
title: "AI 입문 번외편: 머신러닝과 딥러닝의 기초, 트랜스포머와의 연계 및 실전 적용"
categories: "AI"
tag: "Explanation"
toc: true
author_profile: false
sidebar:
nav: "docs"
---

# AI 입문 시리즈 (번외편)

지난 10편의 시리즈에서 우리는 트랜스포머의 전체 생애주기(기초 구성, 특징, 최적화, 변형, Fine-Tuning, 배포)를 수식, 코드, 그리고 실전 팁을 통해 깊이 있게 알아보았다.  
이번 번외편에서는 한 걸음 뒤로 물러나, 트랜스포머가 서 있는 거대한 지반인 머신러닝(ML)과 딥러닝(DL)의 기초를 다진다.  
머신러닝의 전통적인 알고리즘(선형 회귀, SVM, 결정 트리)부터 딥러닝의 핵심 신경망(MLP, CNN, RNN)까지, 이들의 배경을 알아야 트랜스포머의 혁신성이 어디에서 비롯되었는지 명확히 이해할 수 있다.  
특히 이번 편에서는 수식을 풍부하게 사용하면서 설명해 보겠다.  
이를 통해 머신러닝의 지도·비지도 학습, 딥러닝의 역전파(Backpropagation)가 트랜스포머의 어텐션 메커니즘과 어떻게 연결되는지 깊이 있게 파고들어 보자.  

---

## 0. 이번 편의 핵심

* 머신러닝(Machine Learning): 기계가 데이터로부터 패턴을 스스로 학습하는 기술이다. 크게 **지도학습**, **비지도학습**, **강화학습**으로 분류된다. 트랜스포머는 머신러닝이라는 큰 틀 안에 있는 딥러닝의 한 분야이다.  
* 딥러닝(Deep Learning): 여러 개의 층을 쌓은 다층 신경망(Deep Neural Network)을 사용해 데이터 속 복잡한 특징을 자동으로 추출하는 기술이다. 트랜스포머는 현대 딥러닝을 대표하는 가장 중요한 아키텍처 중 하나이다(1~10편 참고).  
* 트랜스포머와의 연계: 머신러닝의 전통 알고리즘을 기반으로 딥러닝이 발전했으며, 트랜스포머는 특히 시퀀스 데이터 처리 분야에서 기존의 RNN, CNN 등을 대체하며 새로운 표준이 되었다(8편 참고).  
* 수식과 미분의 역할: 머신러닝은 손실 함수(오차)를 **미분**하여 최적의 모델 파라미터를 찾는다(9편 최적화). 딥러닝의 역전파는 미분의 연쇄 법칙(Chain Rule)을 통해 복잡한 신경망을 효율적으로 학습시키는 핵심 원리이다(2편 참고).  
* 적분 관점의 이해: 학습 과정을 '연속적인 변화'라는 적분의 관점에서 바라보며, 트랜스포머 모델이 어떻게 지식을 누적하며 성장하는지 직관적으로 이해한다.  

---

## 1. 머신러닝의 배경: 데이터 기반 패턴 학습  

### 1.1 머신러닝이란 무엇인가  

머신러닝(ML)은 명시적인 규칙을 프로그래밍하는 대신, 데이터로부터 모델을 학습시켜 예측, 분류, 군집화(클러스터링) 등의 작업을 수행하는 인공지능의 한 분야이다.  
2편에서 다룬 경사 하강법(Gradient Descent)은 머신러닝 모델의 심장과 같은 핵심 최적화 기법이다.  

머신러닝은 학습 데이터의 형태에 따라 크게 세 가지로 나뉜다.  
* 지도 학습 (Supervised Learning): 정답(Label)이 있는 데이터로 학습한다. (예: 스팸 메일 분류)  
* 비지도 학습 (Unsupervised Learning): 정답이 없는 데이터에서 숨겨진 구조나 패턴을 찾아낸다. (예: 고객 그룹 분류)  
* 강화 학습 (Reinforcement Learning): 보상(Reward)을 최대로 받는 방향으로 행동을 학습한다. (예: 알파고)  

#### 선형 회귀 (Linear Regression)  
선형 회귀는 데이터의 관계를 가장 잘 설명하는 '직선'을 찾는 문제이다.  
예측값($\hat{y}$)은 입력 데이터($\mathbf{X}$)와 가중치($\mathbf{w}$), 편향($b$)의 관계로 표현된다.  
$$ \hat{y} = \mathbf{X}\mathbf{w} + b $$
모델의 목표는 실제 정답($y$)과 예측값($\hat{y}$)의 차이, 즉 **손실(Loss)**을 최소화하는 것이다.  
이때 평균 제곱 오차(MSE, Mean Squared Error)가 손실 함수로 주로 사용된다.  
$$ L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

* **계산 과정**:  
    1.  가중치 $\mathbf{w}$를 임의의 값(예: 0)으로 초기화한다.  
    2.  손실 함수 $L$을 $\mathbf{w}$에 대해 미분하여 기울기($\frac{\partial L}{\partial \mathbf{w}}$)를 구한다. 이 기울기는 오차를 가장 가파르게 줄일 수 있는 방향을 알려준다.  
        $$ \frac{\partial L}{\partial \mathbf{w}} = \frac{2}{n} \mathbf{X}^T (\mathbf{X}\mathbf{w} + b - y) $$
    3.  가중치를 기울기의 반대 방향으로 아주 조금씩 업데이트한다. 이 과정을 손실이 더 이상 줄어들지 않을 때까지 반복한다.  
        $$ \mathbf{w} \leftarrow \mathbf{w} - \eta \frac{\partial L}{\partial \mathbf{w}} $$
        (여기서 $\eta$는 학습률(learning rate)로, 얼마나 '조금씩' 업데이트할지를 결정한다.)  
* **의미**: 이 과정은 마치 안개 속에서 산을 내려올 때, 가장 경사가 가파른 방향으로 한 발씩 내딛는 것과 같다. 최종적으로 손실이 가장 낮은 골짜기(최적의 가중치 $\mathbf{w}$)에 도달하게 된다.  

머신러닝은 **"공장 생산 라인"**과 같다.  
* **데이터(원료)**가 투입되면, **모델(기계)**이 이를 가공하여 **예측(제품)**을 만들어낸다.  
* 이때 트랜스포머는 매우 정교하고 복잡한 시퀀스(문장, DNA 등)라는 원료를 처리할 수 있는 최첨단 생산 라인이다(8편 참고).  

**[ML의 특성]**
* **장점**: 딥러닝에 비해 모델 구조가 단순하여 **해석 가능성**이 높고, 비교적 **적은 데이터**로도 준수한 성능을 낸다.  
* **단점**: 학습 데이터에만 너무 치우쳐 새로운 데이터를 잘 예측하지 못하는 **과적합(Overfitting)** 문제가 발생하기 쉽다. 이는 9편에서 다룬 가중치 감쇠(Weight Decay)와 같은 정규화 기법이나 교차 검증(Cross-validation)으로 완화할 수 있다.  

### 1.2 미분과 ML의 연결  

미분은 머신러닝 학습의 **나침반**이다.  
손실 함수를 미분한 값, 즉 기울기($\nabla L$)는 현재 위치에서 손실이 가장 커지는 방향을 알려준다.  
따라서 모델은 이 기울기의 **반대 방향**으로 파라미터를 업데이트하여 손실을 최소화한다.

* **선형 회귀**에서는 $\frac{\partial L}{\partial \mathbf{w}} = \frac{2}{n} \mathbf{X}^T (\mathbf{X}\mathbf{w} - y)$를 통해 가중치 $\mathbf{w}$를 조정한다.  
* **SVM (Support Vector Machine)**에서는 힌지 손실($L = \max(0, 1 - y(\mathbf{w}\mathbf{x} + b))$)을 미분하기 위해 서브그레디언트(subgradient)라는 기법을 사용한다.  

미분은 "오르막길의 경사도"를 알려주는 도구이며, 머신러닝은 이 도구를 사용해 "가장 효율적인 기계 조립 경로(최적의 파라미터)"를 찾아낸다.  

* **장단점**: 
    * 장점: 선형 모델이나 SVM은 구조가 간단하고 학습 속도가 빠르다.  
    * 단점: 데이터의 패턴이 복잡한 비선형 구조일 경우 성능이 급격히 저하된다.  
* **변형**: SVM에 커널 트릭(Kernel Trick)을 적용하면 비선형 데이터도 효과적으로 분류할 수 있다.  
* **실전 팁**: Scikit-learn 라이브러리의 `LinearRegression`으로 쉽게 구현할 수 있다. 데이터 1,000개, 학습률 $\eta=0.01$ 정도로 시작하며 실험하는 것이 일반적이다.  
* **결정 트리(Decision Tree)**: 데이터를 가장 잘 나눌 수 있는 질문(기준)을 찾아나가는 모델이다. 이때 '지니 불순도($\sum p_k (1 - p_k)$)'를 낮추는 방향으로 분할을 진행한다.  

---

## 2. 머신러닝 알고리즘 상세: 선형 모델부터 앙상블까지

### 2.1 선형 회귀와 로지스틱 회귀

* **선형 회귀 (Linear Regression)**: 집값, 주가처럼 **연속적인 값**을 예측하는 데 사용된다.  
    * 수식 (손실 함수 MSE): $L = \text{MSE} = \frac{1}{n} \|\mathbf{y} - \mathbf{Xw}\|_2^2$
    * 미분 유도: $\frac{\partial L}{\partial \mathbf{w}} = -\frac{2}{n} \mathbf{X}^T (\mathbf{y} - \mathbf{Xw})$ 이며, $\mathbf{w} \leftarrow \mathbf{w} - \eta \frac{\partial L}{\partial \mathbf{w}}$ 규칙으로 가중치를 업데이트한다.  
    * 의미: 각 입력 특성(방의 개수, 면적 등)이 결과(집값)에 얼마나 중요한 영향을 미치는지 **가중치($\mathbf{w}$)**를 통해 학습한다.  

* **로지스틱 회귀 (Logistic Regression)**: 스팸/정상, 합격/불합격처럼 두 가지 중 하나를 결정하는 **이진 분류**에 사용된다.  
    * 선형 회귀의 예측값($\mathbf{Xw}+b$)을 시그모이드(Sigmoid) 함수에 통과시켜 0과 1 사이의 확률값으로 변환한다.  
        $$ \sigma(z) = \frac{1}{1 + e^{-z}} $$
    * 수식 (손실 함수, Cross-Entropy): $L = -\frac{1}{n} \sum [y \log(\sigma) + (1-y) \log(1-\sigma)]$
    * 미분: $\frac{\partial L}{\partial \mathbf{w}} = \frac{1}{n} \mathbf{X}^T (\sigma(\mathbf{Xw}) - y)$. 여기서 시그모이드 함수의 미분($\sigma' = \sigma(1-\sigma)$)이 연쇄 법칙에 따라 적용된다.  

선형 모델이 **"단순한 직선 도로 주행"**이라면, 트랜스포머는 어텐션 메커니즘을 통해 데이터 간의 복잡한 관계를 파악하는 **"거미줄처럼 얽힌 고속도로망을 자율 주행"**하는 것과 같다.  

### 2.2 SVM과 결정 트리

* **SVM (Support Vector Machine)**: 데이터를 가장 잘 구분하는 '경계선(결정 경계)'을 찾는 알고리즘이다.  \
특징은 경계선과 가장 가까운 데이터(서포트 벡터) 사이의 **거리(마진)를 최대화**한다는 점이다.  
    * 수식 (최적화 문제): $\min \frac{1}{2}\|\mathbf{w}\|^2 + C \sum \xi_i$ 라는 조건 하에, $y_i(\mathbf{w}\mathbf{x}_i + b) \ge 1 - \xi_i$를 만족해야 한다.
    * 의미: 마진을 최대화함으로써 새로운 데이터에 대해서도 안정적으로 분류할 수 있는 일반화 성능을 높인다.  

* **결정 트리 (Decision Tree)**: '스무고개'처럼 일련의 질문을 통해 데이터를 분류한다.  
각 질문은 데이터의 **불순도(Impurity)**를 가장 많이 낮출 수 있는 기준으로 선택된다.  
    * 수식 (불순도 지표, 엔트로피): $H = -\sum p \log p$
    * 미분: 결정 트리는 미분 기반이 아니지만, 여러 트리를 결합한 **그래디언트 부스팅(Gradient Boosting)** 모델(예: XGBoost)은 미분을 통해 이전 트리의 오차를 보완하는 새로운 트리를 학습한다.  

* **장단점**: SVM은 마진 최대화 덕분에 과적합에 강하지만, 대규모 데이터에서는 학습이 느리다.  
결정 트리는 결과 해석이 쉽지만, 과적합되기 쉬워 **랜덤 포레스트(Random Forest)**와 같은 앙상블 기법으로 보완한다.  
* **실전 팁**: Scikit-learn의 `SVC(kernel='rbf', C=1)`은 비선형 분류에 강력하다.  
결정 트리는 과적합을 막기 위해 `max_depth=10`과 같이 깊이를 제한하는 것이 중요하다.  

### 2.3 비지도 학습: 클러스터링과 PCA  

* **K-평균 군집화 (K-Means Clustering)**: 정답 없이 주어진 데이터를 K개의 그룹으로 묶는 알고리즘이다.  
    * 수식 (최적화 목표): $L = \sum_{j=1}^{K} \sum_{i \in S_j} \|\mathbf{x}_i - \mu_j\|^2_2$
    * 계산: 각 데이터 포인트를 가장 가까운 군집 중심($\mu_j$)에 할당하고, 다시 군집의 중심으로 중심점을 이동시키는 과정을 반복한다.  

* **주성분 분석 (PCA, Principal Component Analysis)**: 데이터의 **분산을 가장 잘 설명하는** 새로운 축(주성분)을 찾아 고차원 데이터를 저차원으로 압축하는 **차원 축소** 기법이다.  
    * 계산: 데이터의 공분산 행렬을 고유값 분해(Eigenvalue Decomposition)하여 주성분을 찾는다.  

\비지도 학습이 **"라벨 없는 물건들을 자동으로 분류하는 컨베이어 벨트"**라면, 트랜스포머의 자기지도학습(Self-supervised Learning) 방식인 BERT의 MLM(Masked Language Model)은 **"문장의 일부를 가리고 스스로 정답을 만들며 학습하는 고성능 컨베이어 벨트"**와 같다(8편 참고).  

---

## 3. 딥러닝의 배경: 다층 신경망과 특징 추출

### 3.1 딥러닝이란 무엇인가

딥러닝은 여러 층의 인공 신경망(Artificial Neural Network, ANN)을 깊게 쌓아, 데이터 속에 숨겨진 **복잡한 비선형적 특징을 자동으로 학습**하는 기술이다.  
이는 머신러닝의 확장된 분야이며, 트랜스포머는 딥러닝의 최신 성과를 대표한다(1편 참고).  
딥러닝의 핵심 학습 원리는 **역전파(Backpropagation)**이다.  

다층 퍼셉트론 (MLP, Multi-Layer Perceptron)  
MLP는 입력층, 여러 개의 은닉층, 출력층으로 구성된 가장 기본적인 신경망이다.  
각 층은 이전 층의 출력에 가중치($\mathbf{W}_l$)를 곱하고 편향($\mathbf{b}_l$)을 더한 뒤, 활성화 함수($\sigma$)를 적용하여 다음 층으로 전달한다.  
$$ \mathbf{h}_l = \sigma(\mathbf{W}_l \mathbf{h}_{l-1} + \mathbf{b}_l) $$
* **계산 과정**:
    1.  **순전파 (Forward Propagation)**: 입력 데이터 $\mathbf{x}$가 첫 번째 층부터 마지막 층까지 순서대로 통과하며 예측값 $\hat{y}$를 계산한다.  
    2.  **역전파 (Backpropagation)**: 마지막 층에서 계산된 오차(Loss)를 기반으로, 연쇄 법칙(Chain Rule)을 이용해 각 층의 가중치가 오차에 얼마나 기여했는지($\delta_l$)를 역으로 계산해 나간다.  
        $$ \delta_L = \text{softmax}' (\mathbf{h}_L - \mathbf{y}) $$
        $$ \delta_l = (\mathbf{W}_{l+1}^T \delta_{l+1}) \odot \sigma'(\mathbf{h}_l) $$
        (여기서 $\odot$는 행렬의 원소별 곱셈을 의미한다.)  
* **의미**: 신경망의 층이 깊어질수록, 저수준의 단순한 특징(선, 모서리)에서 고수준의 복잡하고 추상적인 특징(눈, 코, 얼굴 전체)으로 **특징이 계층화**되어 학습된다.  

딥러닝은 **"자동차 조립 라인"**에 비유할 수 있다.  
* 머신러닝이 단일 부품(엔진)을 만드는 과정이라면, 딥러닝은 여러 공정(층)을 거쳐 차체, 엔진, 바퀴 등을 순서대로 조립해 완성차(최종 예측)를 만드는 것과 같다.  
* MLP, CNN을 거쳐 트랜스포머로 발전하는 과정은, 수동 조립 라인이 자동화, 컨베이어 벨트 시스템을 거쳐 완전 자율화된 스마트 팩토리로 진화하는 모습에 해당한다.  

**DL의 특성**
* **장점**: 매우 복잡한 패턴을 학습할 수 있어 이미지, 음성, 자연어 처리 등에서 압도적인 성능을 보인다.  
* **단점**: **대규모 데이터**와 고성능 GPU 연산이 필수적이며(9편 Chinchilla 법칙 참고), 학습 과정이 소멸하는 **기울기 소실(Vanishing Gradient)** 문제에 취약하다(8편 LSTM 참고).  

### 3.2 미분과 DL의 연결

역전파는 **미분의 연쇄 법칙(Chain Rule)을 자동화**한 알고리즘이다.  
최종 손실 $L$에 대한 각 층의 출력 $\mathbf{h}_l$의 미분값($\delta_l = \frac{\partial L}{\partial \mathbf{h}_l}$)은, 바로 다음 층의 미분값($\frac{\partial L}{\partial \mathbf{h}_{l+1}}$)을 이용해 연쇄적으로 계산된다.  
$$ \frac{\partial L}{\partial \mathbf{h}_l} = \frac{\partial L}{\partial \mathbf{h}_{l+1}} \frac{\partial \mathbf{h}_{l+1}}{\partial \mathbf{h}_l} $$
이 과정은 "복잡한 조립 라인의 최종 불량품(오차)에 대한 원인을 각 공정(층)별로 역추적하여 책임을 할당하는" 것과 같다.  
딥러닝은 이 미분값을 이용해 수백만 개의 파라미터를 동시에 최적화한다.  

* **장단점**:
    * 장점: 이미지 분류에서 99% 이상의 정확도를 달성하는 등 복잡한 패턴 학습에 매우 강력하다.  
    * 단점: 모델 내부가 복잡해 왜 그런 결정을 내렸는지 해석하기 어려운 **블랙박스(Black-box)** 문제가 있다.  
PyTorch의 `nn.Module`을 상속받아 모델을 설계하고, 9편에서 다룬 Adam과 같은 옵티마이저와 배치 크기 64 정도로 학습을 시작하는 것이 일반적이다.  

---

## 4. 딥러닝 아키텍처: MLP부터 CNN/RNN까지

### 4.1 MLP와 CNN

* **MLP (Multi-Layer Perceptron)**: 모든 뉴런이 다음 층의 모든 뉴런과 연결된 **완전 연결(Fully-connected)** 구조이다.  
파라미터 수가 층의 깊이와 뉴런 수에 따라 기하급수적으로($O(d^2 L)$) 증가한다.  

* **CNN (Convolutional Neural Network)**: 이미지 처리를 위해 탄생한 아키텍처다.  
**컨볼루션(Convolution) 연산**을 통해 이미지의 지역적 특징(local feature)을 추출한다.  
    * 수식 (컨볼루션): $y[i] = \sum_k W[k] x[i-k] + b$
    * **가중치 공유(Weight Sharing)**: 작은 크기의 필터(커널)가 이미지 전체를 훑으며 동일한 가중치를 공유하기 때문에 MLP보다 파라미터 수가 훨씬 적고 효율적이다.  
    * 미분: 컨볼루션 연산의 역전파는 뒤집힌 필터를 이용한 컨볼루션으로 계산된다.  

CNN이 이미지의 **"지역적 패턴을 찾아내는 돋보기(필터)"** 역할이라면, 트랜스포머 기반의 ViT(Vision Transformer)는 이미지 전체를 조망하며 **"전역적인 관계를 파악하는 위성 사진"**과 같다(10편 참고).  

### 4.2 RNN과 LSTM  

* **RNN (Recurrent Neural Network)**: 이전 타임스텝의 출력($\mathbf{h}_{t-1}$)이 현재 타임스텝의 입력으로 다시 들어가는 **순환 구조**를 가져 시퀀스 데이터 처리에 적합하다.  
    * 수식 (RNN): $\mathbf{h}_t = \tanh(\mathbf{W}_h \mathbf{h}_{t-1} + \mathbf{W}_x \mathbf{x}_t)$  
    * 미분 (BPTT, Backpropagation Through Time): 시간을 거슬러 역전파를 수행하며, 시퀀스가 길어질수록 기울기가 0으로 수렴하는 **기울기 소실(Vanishing Gradient)** 문제가 발생한다.  

* **LSTM (Long Short-Term Memory)**: RNN의 장기 의존성 문제를 해결하기 위해 등장했다.  
**입력, 망각, 출력 게이트(Gate)**라는 장치를 통해 어떤 정보를 기억하고, 잊고, 출력할지를 제어한다.  
    * 수식 (핵심, Cell State): $\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tanh(\dots)$  
    * 미분: 셀 상태($\mathbf{c}_t$)는 곱셈이 아닌 덧셈 연산으로 연결되어 기울기가 소멸하지 않고 오래도록 전달될 수 있다. (8편 트랜스포머와 비교 참고).  

* **장단점**: RNN은 시퀀스 처리에 강점이 있지만 기울기 소실 문제가 치명적이다.  
LSTM과 GRU(Gated Recurrent Unit)는 이를 개선했지만, 순차적 연산 때문에 병렬 처리가 어려워 학습 속도가 느리다.  

### 4.3 트랜스포머와의 연계  

트랜스포머는 RNN의 순차적 구조를 과감히 버리고 **어텐션(Attention) 메커니즘**만으로 시퀀스 내 모든 단어 간의 관계를 한 번에 계산한다.  
이로써 **병렬화**가 가능해져 학습 속도를 획기적으로 개선했으며, 딥러닝 시대의 정점으로 자리 잡았다(5편 참고).  

결국 트랜스포머의 성공은 **머신러닝의 최적화 원리(9편)**와 **딥러닝의 역전파 기반 학습(2편)**이라는 두 거인의 어깨 위에 서 있기에 가능했다.  

---

## 5. ML/DL과 트랜스포머 비교 (성능·연산·적용)  

### 5.1 비교 수식과 장단점  

* **ML (선형 모델)**: 연산 복잡도 $O(d)$. 모델이 단순해 해석이 쉽다.  
* **DL (MLP)**: 연산 복잡도 $O(d^2 L)$. 특징을 자동으로 학습하지만 해석이 어렵다.  
* **트랜스포머**: 연산 복잡도 $O(N^2 d)$ (어텐션). 시퀀스 길이에 제곱으로 비례해 계산량이 많지만, 시퀀스 데이터에서 최고의 성능을 보인다.  

| 측면 | ML (SVM) | DL (CNN) | 트랜스포머 |
| :--- | :--- | :--- | :--- |
| **연산 복잡도** | $O(n \cdot d)$ | $O(n \cdot k \cdot d)$ | $O(N^2 \cdot d)$ |
| **데이터 요구량** | 중간 | 높음 | 매우 높음 |
| **해석 가능성** | 높음 | 중간 | 낮음 |
| **주요 적용 분야** | 정형 데이터 분류 | 이미지 | 자연어, 시퀀스 |

모델의 발전 과정을 자동차에 비유하면 다음과 같다.  
* **ML**: 운전자가 모든 것을 제어하는 **"수동 기어"** 자동차.  
* **DL**: 기본적인 주행은 자동화된 **"자동 기어"** 자동차.  
* **트랜스포머**: 복잡한 상황을 스스로 판단하고 처리하는 **"자율 주행"** 자동차.  

---

## 6. 숫자로 따라가는 미니 예제  

**설정**: 선형 회귀, 데이터 개수 $n=3$, 입력 $\mathbf{X}=[[1,2],[3,4],[5,6]]$, 정답 $\mathbf{y}=[3,7,11]$, 학습률 $\eta=0.01$, 반복 $t=1$.  

1.  **초기화**: 가중치 $\mathbf{w}=[0,0]^T$, 편향 $b=0$.  
2.  **예측**: $\hat{\mathbf{y}} = \mathbf{Xw} = [0,0,0]^T$.  
3.  **기울기 계산**: $\frac{\partial L}{\partial \mathbf{w}} = \frac{2}{3} \mathbf{X}^T (\hat{\mathbf{y}} - \mathbf{y})$.  
    * $\hat{\mathbf{y}} - \mathbf{y} = [-3, -7, -11]^T$.  
    * $\mathbf{X}^T (\hat{\mathbf{y}} - \mathbf{y}) = \begin{bmatrix} 1 & 3 & 5 \\ 2 & 4 & 6 \end{bmatrix} \begin{bmatrix} -3 \\ -7 \\ -11 \end{bmatrix} = \begin{bmatrix} -79 \\ -100 \end{bmatrix}$.  
    * $\frac{\partial L}{\partial \mathbf{w}} = \frac{2}{3} \begin{bmatrix} -79 \\ -100 \end{bmatrix} \approx \begin{bmatrix} -52.67 \\ -66.67 \end{bmatrix}$.  
4.  **가중치 업데이트**: $\mathbf{w} \leftarrow \mathbf{w} - \eta \frac{\partial L}{\partial \mathbf{w}}$.  
    * $\mathbf{w}_{\text{new}} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} - 0.01 \times \begin{bmatrix} -52.67 \\ -66.67 \end{bmatrix} = \begin{bmatrix} 0.5267 \\ 0.6667 \end{bmatrix}$.  
    * 첫 번째 학습만으로 가중치가 0에서 업데이트된 것을 확인할 수 있다.  

---

## 7. 그래프와 시각화  

* **손실 곡선 (Loss Curve)**: 학습이 진행됨에 따라 손실이 어떻게 감소하는지 보여준다. 일반적으로 ML은 완만하게, DL은 더 빠르고 복잡한 곡선을 그리며 하강한다.  
* **결정 트리 시각화**: `Graphviz` 같은 도구를 사용하면 트리가 어떤 기준으로 데이터를 분할하는지 명확하게 볼 수 있다.  
* **CNN 필터 시각화**: 학습된 CNN 필터를 시각화하면, 모델이 이미지의 어떤 패턴(예: 수직선, 특정 색상)에 반응하는지 알 수 있다.  

---

## 8. 적분 관점의 이해 (심화)  

학습 과정을 미분이 아닌 적분의 관점에서 바라보면 새로운 통찰을 얻을 수 있다.  

* **ML**: 가중치 업데이트 과정은 기울기($-\eta \nabla L$)를 시간에 따라 계속 더해나가는 **적분**으로 볼 수 있다.  
    $$ 
    \mathbf{w}(t) \approx \int -\eta \nabla L(\mathbf{w}) dt 
    $$  
* **DL**: 신경망의 순전파는 각 층의 변환($\sigma(\mathbf{W}\mathbf{h})$)이 **누적**되는 과정이다.  
    $$ 
    \mathbf{h}_{\text{out}} \approx \int \sigma(\mathbf{W}(l)\mathbf{h}(l)) dl 
    $$  
* **트랜스포머**: 어텐션은 시퀀스 전체($N$)에 대한 정보를 가중합하여 **통합**하는 과정이다.  
    $$ 
    \text{Attention} \approx \int \text{softmax}(\dots) \mathbf{V} dN 
    $$  

이 관점에서 학습은 **"강물의 흐름"**과 같다.  
* **ML**: 정해진 경로를 따라 흐르는 **직선적인 강**.  
* **DL**: 여러 지류가 합쳐지고 복잡하게 굽이치는 **굴곡진 강**.  
* **트랜스포머**: 강 전체의 모든 물방울이 서로 상호작용하며 거대한 네트워크를 이루는 **복잡한 수계망**.  

---

## 9. 코드 확인 (Scikit-learn & PyTorch 의사코드)  

```python
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
import torch
import torch.nn as nn

# --- 1. 머신러닝: Scikit-learn ---

# 선형 회귀 예제
# 데이터 준비
X_ml = [[1,2],[3,4],[5,6]]
y_ml = [3,7,11]
# 모델 생성 및 학습
model = LinearRegression()
model.fit(X_ml, y_ml) # fit 한 줄이면 학습 끝
# 예측
print(f"ML Predicted: {model.predict([[7,8]])}")

# SVM 예제
svm = SVC(kernel='linear', C=1)
# 라벨은 0 또는 1로 가정
svm.fit(X_ml, [0,1,0])
print(f"SVM Predicted: {svm.predict([[7,8]])}")

# --- 2. 딥러닝: PyTorch ---

# MLP 모델 정의
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2, 4) # 입력 2, 출력 4
        self.fc2 = nn.Linear(4, 1) # 입력 4, 출력 1
    
    def forward(self, x):
        # 순전파 로직: fc1 -> ReLU 활성화 -> fc2
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# 모델, 옵티마이저, 손실함수 설정
mlp = MLP()
optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

# 가상의 학습 루프 (1회)
input_tensor = torch.tensor([[1.,2.]])
target_tensor = torch.tensor([[3.]]) # 목표값

# 순전파
output = mlp(input_tensor)
# 손실 계산
loss = loss_fn(output, target_tensor)
# 역전파
loss.backward()
# 파라미터 업데이트
optimizer.step()

print(f"DL MLP Output: {output.detach().numpy()}")
```  

  * **Scikit-learn (ML)**: `fit()`과 `predict()` 두 개의 메소드로 학습과 예측이 매우 직관적으로 이루어진다.  
  * **PyTorch (DL)**: `nn.Module`로 모델 구조를 직접 설계하고, `loss.backward()`와 `optimizer.step()`을 통해 역전파와 파라미터 업데이트를 명시적으로 수행하는 유연한 구조를 가진다.  

-----

## 10\. 자주 헷갈리는 포인트  

  * **ML vs DL**: 머신러닝은 딥러닝을 포함하는 더 넓은 개념이다. 딥러닝은 '깊은 신경망'을 사용하는 머신러닝의 한 분야를 지칭한다.  
  * **과적합 해결**: ML에서는 주로 L1/L2 정규화(가중치 크기 제한)를, DL에서는 드롭아웃(Dropout, 학습 시 뉴런 일부를 비활성화)을 추가로 사용한다.  
  * **트랜스포머의 위치**: 트랜스포머는 딥러닝 아키텍처의 일종으로, 특히 시퀀스 데이터 처리에 특화된 모델이다.  

-----

## 11\. 실무 감각  

  * **하이퍼파라미터 튜닝**: 모델 성능에 큰 영향을 미치는 값들이다. ML에서는 SVM의 `C` 값, DL에서는 학습률(`lr=1e-3`), 배치 크기(`32~256`) 등이 대표적이다.  
  * **디버깅**: ML 모델이 과적합되면 교차 검증 점수를 확인한다. DL 모델 학습 중 손실이 `NaN`(Not a Number)으로 폭주하면 그래디언트 클리핑(Gradient Clipping)을 적용해볼 수 있다.  
  * **학습 과정 모니터링**: Scikit-learn의 `metrics` 모듈로 성능을 평가하고, PyTorch는 `TensorBoard`와 연동하여 손실 곡선과 파라미터 분포를 시각적으로 확인하는 것이 필수적이다.  

-----

## 이번 편 요약  

  * **머신러닝**: 데이터로부터 패턴을 배우는 기술. 지도학습(선형 회귀, $L=\\text{MSE}$), 비지도학습(K-Means) 등이 있으며, 경사 하강법으로 최적화한다.  
  * **딥러닝**: 다층 신경망을 이용한 기술. 역전파($\\delta\_l = (\\mathbf{W}^T \\delta\_{l+1}) \\odot \\sigma'$)가 핵심 원리이며 MLP, CNN, RNN 등의 아키텍처가 있다.  
  * **트랜스포머와의 관계**: 딥러닝의 한 분야로, 어텐션 메커니즘을 통해 기존 RNN의 한계를 극복하고 시퀀스 처리의 새로운 패러다임을 열었다.  
  * **핵심 원리, 미분**: 머신러닝과 딥러닝 모두 미분을 통해 오차를 줄여나가는 최적화 과정을 기반으로 한다.  

-----

## 시리즈 전체 마무리 및 결론

AI 입문 시리즈 1편부터 10편, 그리고 이번 번외편까지 우리는 머신러닝과 딥러닝의 기초부터 시작해 트랜스포머라는 현대 AI의 정수까지 포괄적으로 탐험했다.  
머신러닝은 **패턴 학습**의 기반을, 딥러닝은 **특징 학습의 자동화**를, 그리고 트랜스포머는 **시퀀스 데이터 처리의 혁신**을 이뤄냈다.  
문제의 종류와 데이터 규모에 따라 Scikit-learn(간단한 ML), PyTorch(유연한 DL), Hugging Face(사전 학습된 트랜스포머) 라이브러리를 조합하여 사용하는 것이 가장 효율적이다.  
이 시리즈가 AI라는 광활한 생태계를 탐험하는 데 든든한 나침반이 되기를 바란다.