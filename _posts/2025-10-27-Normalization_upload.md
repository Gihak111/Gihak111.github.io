---
layout: single
title:  "딥러닝의 정규화 (Regularization)"
categories: "AI"
tag: "linear algebra"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## 딥러닝의 정규화  
딥러닝 모델을 학습시킬 때 가장 조심해야 할 개념 중 하나이다.  
모델이 학습 데이터에만 너무 익숙해져서, 새로운 데이터를 만났을 때 성능이 떨어지는 현상, 즉 '과적합(Overfitting)'을 막기 위한 핵심 기술이기 때문이다.  
예를 들어, 학습 데이터(문제집)로는 99점을 받는데, 실제 시험(테스트 데이터)에서는 50점을 받는 상황을 수학적으로 방지하는 데 사용된다.  
이 문제를 해결하는 핵심 아이디어는 '모델을 의도적으로 단순하게 만들거나 안정화시키는 것'이다.  

## 1. 정규화란 무엇인가?  
모델이 학습 데이터의 아주 사소한 노이즈(noise)까지 전부 암기하려 할 때(즉, 모델이 너무 복잡해질 때), 의도적으로 페널티(Penalty)를 부과하거나 학습 과정을 안정화시켜 모델을 좀 더 단순하고 일반적인 형태로 만드는 모든 기법을 말한다.  

핵심 개념은 다음과 같이 정리할 수 있다.  

- 핵심 문제: 과적합 (Overfitting). 모델이 학습 데이터만 암기한다.  
- 해결 방식:  
    1.  모델의 복잡도(주로 가중치의 크기)에 비례하는 페널티를 손실 함수(Cost Function)에 추가한다. (L1, L2)  
    2.  학습 과정에서 계층을 통과하는 값(활성화 값) 자체를 안정화시킨다. (Batch Norm, Layer Norm)  
- 목표: 학습 데이터와 테스트 데이터 모두에서 좋은 성능을 내는 '일반화(Generalization)' 성능을 높인다.  

## 2. 가중치 정규화와 선형대수학 (L1, L2)  
딥러닝에서 '모델이 복잡하다'는 것은 보통 '가중치(Weight) 값들이 불필요하게 너무 크다'는 의미다. 이 가중치들은 선형대수학의 벡터(Vector) 또는 행렬(Matrix)로 표현된다.  

L1, L2 정규화는 바로 이 가중치 벡터의 '크기'를 측정해서 페널티로 사용한다. 이때 벡터의 크기를 재는 방법이 바로 선형대수학의 놈(Norm)이다.  

- L2 정규화 (Ridge)  
    가중치 벡터의 L2 Norm (유클리드 거리)의 제곱을 페널티로 사용한다.  
    $$Cost = \text{Loss} + \lambda \sum w_i^2$$
    식이 복잡해 보이지만, 원래의 손실(Loss)에 모든 가중치($w$)들의 '제곱의 합'을 더하는 것이다.  
    가중치가 조금만 커져도 페널티($\lambda \sum w_i^2$)가 기하급수적으로 커지기 때문에, 모델은 가중치들을 전반적으로 작게 유지하려고 노력한다. (Weight Decay라고도 불린다.)  

- L1 정규화 (Lasso)
    가중치 벡터의 L1 Norm (맨해튼 거리)을 페널티로 사용한다.  
    $$Cost = \text{Loss} + \lambda \sum |w_i|$$
    이 식은 가중치들의 '절댓값의 합'을 더한다.  
    L2와 달리, 이 방식은 덜 중요한 가중치를 아예 0으로 만들어 버리는 경향이 있다.  
    결과적으로 불필요한 특징(Feature)을 골라내는(Selection) 효과를 준다.  

결국 L2는 가중치들을 전반적으로 작게 만드는 방식, L1은 불필요한 가중치를 아예 0으로 만들어 모델을 더 단순화(희소하게)하는 방식이라고 볼 수 있다.  

## 3. 활성화 값 정규화 (Batch Norm, Layer Norm)  
L1, L2가 모델의 가중치(Weight)를 직접 제한하는 방식이라면, 어떤 기법들은 학습 도중 모델 내부를 흐르는 값(활성화 값) 자체를 안정화시킨다.  

학습이 진행되면서 이전 계층(Layer)들의 가중치가 변하면, 다음 계층이 받는 입력 값들의 분포가 계속 바뀌게 된다.  
이 현상을 내부 공변량 변화(Internal Covariate Shift)라고 부르는데, 이는 학습 속도를 저해하고 과정을 불안정하게 만든다.  
배치 정규화와 레이어 정규화는 이 문제를 해결하기 위해, 계층을 통과하는 값들을 정규화하는 계층(Layer)을 모델 중간에 삽입한다.  

### 3-1. 배치 정규화 (Batch Normalization)
이름 그대로, 같은 미니배치(Mini-batch)에 속한 여러 데이터들의 동일한 채널(특징) 값을 모아서 평균 0, 분산 1로 정규화한다.  
식이 조금 많아 보이지만, 결국 평균 빼고 표준편차로 나누는 표준 정규 분포화 과정이다.  

1.  미니배치 평균($\mu_B$) 계산:  
    $$\mu_B \leftarrow \frac{1}{m} \sum_{i=1}^m x_i$$
2.  미니배치 분산($\sigma_B^2$) 계산:  
    $$\sigma_B^2 \leftarrow \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2$$
3.  정규화($\hat{x}_i$):  
    $$\hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
4.  스케일 및 시프트($y_i$):  
    $$y_i \leftarrow \gamma \hat{x}_i + \beta$$

-   $m$: 미니배치의 크기 (예: 32개).  
-   $x_i$: 특정 채널(특징)에 대한 $i$번째 데이터의 값.  
-   $\epsilon$: 분모가 0이 되는 것을 막는 아주 작은 값 (e.g., $10^{-5}$).  
-   $\gamma, \beta$: 모델이 스스로 학습하는 파라미터.  

가장 중요한 것은 4번 단계의 $\gamma$(감마)와 $\beta$(베타)이다.  
만약 모델이 판단하기에, 애써 정규화(3단계)한 값이 너무 밋밋해서 특징을 살리기 어렵다면,  
모델은 스스로 $\gamma$ 값을 키워 스케일을 조절하고 $\beta$ 값으로 이동(Shift)시켜 가장 최적화된 분포를 찾아낸다.  

특징: (주로 CNN에서 사용) 학습 속도를 크게 향상시키지만,  
배치 크기($m$)에 의존적이라 배치 크기가 너무 작으면 평균/분산이 불안정해져 성능이 떨어질 수 있다.  

### 3-2. 레이어 정규화 (Layer Normalization)  
배치 정규화의 단점을 보완하기 위해 나왔다. 배치 내의 단일 데이터 내부의 모든 채널(특징) 값을 모아서 평균 0, 분산 1로 정규화한다.  

수학식은 배치 정규화와 거의 동일하지만, 평균과 분산을 구하는 대상이 완전히 다르다.  

1.  레이어 평균($\mu_L$) 계산:  
    $$\mu_L \leftarrow \frac{1}{H} \sum_{i=1}^H x_i$$
2.  레이어 분산($\sigma_L^2$) 계산:  
    $$\sigma_L^2 \leftarrow \frac{1}{H} \sum_{i=1}^H (x_i - \mu_L)^2$$
3.  정규화($\hat{x}_i$):  
    $$\hat{x}_i \leftarrow \frac{x_i - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}}$$
4.  스케일 및 시프트($y_i$):  
    $$y_i \leftarrow \gamma \hat{x}_i + \beta$$

-   $H$: 해당 레이어의 특징(또는 뉴런, 채널)의 총 개수.  
-   $x_i$: '단일 데이터'의 $i$번째 특징 값.  
-   $\gamma, \beta$: 역시 모델이 학습하는 파라미터.  

특징: (주로 RNN, Transformer에서 사용) 이 방식은 배치 크기($m$)와 상관없이 데이터 1개 내부에서만 계산이 완료된다.  
따라서 배치 크기가 1이 되거나 데이터(문장)마다 길이가 달라져도 안정적으로 동작한다.  

## 4. 종합  
정리해 보면,  
1.  상황 정의: 모델이 학습 데이터에 과적합되거나 학습이 불안정한 위험이 있다.  
2.  문제 파악: 원인은 모델의 가중치가 너무 복잡해지거나(L1, L2의 영역), 계층을 통과하는 값의 분포가 불안정하기 때문이다(Batch/Layer Norm의 영역).  
3.  해결 (1) - 가중치 제한: 선형대수학의 'Norm'을 사용해 가중치 벡터의 크기를 측정하고, 이를 손실 함수에 페널티로 추가한다. (L1, L2)  
4.  해결 (2) - 활성화 값 안정화: 모델 중간에 정규화 계층을 삽입하여, 계층으로 들어오는 값의 분포를 안정적으로 재조정한다.  
    -   배치 정규화: (데이터 1, 특징 1), (데이터 2, 특징 1), (데이터 3, 특징 1)... 을 모아서 정규화.  
    -   레이어 정규화: (데이터 1, 특징 1), (데이터 1, 특징 2), (데이터 1, 특징 3)... 을 모아서 정규화.  
5.  결과: 모델이 불필요한 가중치를 줄이고(L1/L2) 학습 과정을 안정화시켜(Batch/Layer Norm) 일반화 성능이 향상된다.  

## 5. 결론  
정규화는 모델의 성능을 안정적으로 끌어올리는 가장 기본적이면서도 강력한 기술들의 집합이다.  
L1, L2가 모델이 엉뚱한 방향으로 과하게 학습하지 않도록 브레이크를 걸어주는 느낌이라면, 배치 정규화나 레이어 정규화는 학습 과정 자체가 안정적인 궤도를 달리도록 도와주는 '안정 장치' 같은 느낌이다.  
이걸 알아야 드롭아웃(Dropout) 같은 또 다른 방식의 정규화 기법으로 넘어갈 수 있다.  