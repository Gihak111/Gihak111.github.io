---
layout: single
title:  "local minima global minimum"
categories: "AI"
tag: "linear algebra"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## 지역 최솟값, 전역 최솟값

경사하강법 하다보며 가자 ㅇ낮은 지점을 찾아 내려가야 한 다는 내용 많이 들었을 것이다.  
제일 낮은 loss를 찾기 위함이다.  
그게 딥러닝의 목표중 하나이다.  
하지만, 가끔, 전역 최솟값으로 수렴하지 못하고, 지역 최솟값으로 수렴 할 때가 있다.  

## 2. 전역 최솟값 vs. 지역 최솟값

손실 함수라는 거대한 지형에는 우리가 목표로 하는 가장 낮은 지점 외에도, 우리를 함정에 빠뜨릴 수 있는 여러 지점이 존재한다.

#### 2.1. 전역 최솟값 (Global Minimum)

손실 함수 전체에서 가장 낮은 손실 값을 갖는 지점이다. 딥러닝 모델이 이론적으로 도달할 수 있는 가장 이상적인 최적의 상태를 의미한다.

#### 2.2. 지역 최솟값 (Local Minimum)

전체적으로는 가장 낮은 지점이 아니지만, 주변의 한정된 지역(local) 내에서는 가장 낮은 손실 값을 갖는 지점이다. 마치 산에서 가장 낮은 계곡인 줄 알았는데, 알고 보니 더 큰 계곡 옆에 있는 작은 웅덩이와 같다.

## 3. 왜 지역 최솟값에 갇히는가?: 경사 하강법의 한계

모델 학습에 사용되는 **경사 하강법(Gradient Descent)**은 현재 위치에서 기울기가 가장 가파른 내리막길로 한 걸음씩 나아가는 단순하고 강력한 전략이다.  

업데이트 수식은 다음과 같다.  

$$W_{\text{new}} = W_{\text{old}} - \eta \nabla L(W_{\text{old}})$$

* $W$: 모델의 가중치
* $\eta$: 학습률 (Learning Rate), 즉 보폭의 크기
* $\nabla L(W_{\text{old}})$: 현재 가중치에서 계산된 손실 함수의 **기울기(Gradient)**

**핵심은 기울기 $\nabla L(W)$ 이다.** 최솟값(전역이든 지역이든)의 수학적 특징은 그 지점에서 **기울기가 0**이라는 것이다. 즉, 사방이 평평하거나 오르막길이라 더 이상 내려갈 곳이 없는 상태를 의미한다.

$$\nabla L(W) = 0$$

만약 모델이 **지역 최솟값**에 도달하면 다음과 같은 일이 발생한다.

1.  해당 지점에서 기울기 $\nabla L(W)$를 계산하면 **0**이 된다.
2.  이 값을 경사 하강법 업데이트 수식에 대입하면, `기울기` 항이 0이 되어 사라진다.
    $$W_{\text{new}} = W_{\text{old}} - \eta \times 0$$
3.  결과적으로 **가중치가 더 이상 업데이트되지 않는다 ($W_{\text{new}} = W_{\text{old}}$)**.

'등산가'는 주변 모든 방향이 오르막길이라고 판단하여, 이곳이 가장 낮은 지점이라 믿고 **움직임을 멈추게 된다.** 이것이 바로 모델이 지역 최솟값에 '갇힌다(stuck)'고 표현하는 이유이다.

## 4. 현대 딥러닝, 왜 전역 최솟값에 집착하지 않는가?

과거에는 이 지역 최솟값 문제가 딥러닝 학습의 큰 난제였지만, 연구가 발전하면서 이제는 다른 관점에서 문제를 바라보게 되었다.  

#### 4.1. 진짜 문제는 '안장점(Saddle Point)'

실제 딥러닝 모델의 가중치는 수백만 개가 넘는 **초고차원(high-dimensional)** 공간에 존재한다.  
이러한 초고차원 공간에서는 움푹 파인 '지역 최솟값'보다, 특정 방향에서는 극솟값이고 다른 방향에서는 극댓값인 **안장점(Saddle Point)**이 훨씬 더 많이 존재한다.  

안장점 역시 기울기가 0이라 경사 하강법을 멈추게 할 수 있지만, **Adam, RMSProp과 같은 최신 옵티마이저(Optimizer)**들은 **관성(Momentum)**과 유사한 개념을 도입하여, 평평한 안장점에서 미끄러져 내려와 탐색을 계속할 수 있도록 설계되었다.  

#### 4.2. '충분히 좋은' 지역 최솟값의 가치

딥러닝의 최종 목표는 훈련 데이터의 손실을 0으로 만드는 것이 아니라, **처음 보는 데이터에 대해서도 예측을 잘하는 일반화(Generalization) 성능**을 높이는 것이다.  

* **과적합(Overfitting)의 위험**: 전역 최솟값은 훈련 데이터에 너무 완벽하게 맞춰진 상태일 수 있다. 이는 오히려 새로운 데이터에 대한 성능을 떨어뜨리는 '과적합'으로 이어질 수 있다. 정답만 외운 학생이 응용 문제에 약한 것과 같다.  
* **성능이 비슷한 평탄한 지역**: 최신 연구에 따르면, 딥러닝 손실 함수에는 수많은 지역 최솟값이 존재하며, 이들 대부분은 전역 최솟값과 거의 차이가 나지 않는 '평탄한(flat)' 지역에 위치한다. 따라서 실용적인 관점에서는 어떤 지역 최솟값에 도달하든 성능 차이가 미미하다.  

## 5. 결론

결론적으로, 현대 딥러닝의 패러다임은 이론적으로 가장 완벽한 '전역 최솟값' 하나를 찾는 것에서, **일반화 성능이 뛰어난 '충분히 좋은 지역 최솟값'**을 효율적으로 찾는 것으로 변화했다.  

Adam과 같은 발전된 최적화 알고리즘 덕분에 안장점이나 지역 최솟점에 갇힐 위험은 크게 줄었으며, 우리는 이제 과적합을 피하면서도 충분히 낮은 손실 값을 갖는 최적점을 찾는 데 집중하고 있다.  