---
layout: single
title:  "Gradient Explosion"
categories: "AI"
tag: "linear algebra"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---



네, 알겠습니다. 제공해주신 'Gradient Descent method' 글의 구조와 스타일을 분석하여, 동일한 저자가 작성한 것처럼 손실 함수 '평균 제곱 오차(Mean Squared Error, MSE)'에 대한 글을 작성해 드리겠습니다.

---
layout: single
title:  "손실 함수 (Loss Function): 평균 제곱 오차 (Mean Squared Error, MSE)"
categories: "AI"
tag: ["loss function", "MSE", "regression"]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## 1. 손실 함수와 MSE의 역할

머신러닝 모델이 학습한다는 것은 '더 나은' 예측을 하는 방향으로 나아가는 과정이다. 그렇다면 모델은 무엇을 기준으로 현재의 예측이 '나쁜지' 혹은 '좋은지'를 판단할 수 있을까? 이 판단의 기준이 되는 척도가 바로 **손실 함수(Loss Function)** 또는 비용 함수(Cost Function)이다.

수많은 손실 함수 중에서, **평균 제곱 오차(Mean Squared Error, MSE)**는 연속된 숫자 값을 예측하는 **회귀(Regression)** 문제에서 가장 널리 사용되는 기본적인 손실 함수이다. MSE는 모델의 예측값이 실제 정답값과 얼마나 차이가 나는지를 측정하는 직관적인 방법을 제공한다.

## 2. 평균 제곱 오차 (MSE)의 정의

평균 제곱 오차는 이름 그대로, 각 데이터에 대한 **오차의 제곱(Squared Error)의 평균(Mean)**을 계산한 값이다. 즉, 예측값과 실제값의 차이를 제곱하여 모두 더한 뒤, 전체 데이터의 개수로 나누어 구한다.

이를 수식으로 표현하면 다음과 같다.

$$J(\theta) = \text{MSE} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$$

이 수식의 각 구성 요소는 다음과 같은 의미를 가진다.

* $m$: 전체 데이터 샘플의 개수
* $x^{(i)}$: $i$번째 입력 데이터
* $y^{(i)}$: $i$번째 데이터의 실제 정답 값 (Label)
* $h_\theta(x^{(i)})$: 모델이 $x^{(i)}$를 입력받아 예측한 값 (Hypothesis)
* $(h_\theta(x^{(i)}) - y^{(i)})$: $i$번째 데이터에 대한 **오차(Error)** 또는 잔차(Residual)
* $\sum_{i=1}^{m} (\dots)^2$: 각 데이터의 **오차를 제곱**하여 모두 더함
* $\frac{1}{m}$: 합산된 값을 데이터 개수로 나누어 **평균**을 구함

결론적으로 MSE 값이 **크다**는 것은 예측값과 실제값의 차이가 크다는 의미이며, 이 값을 최소화하는 것이 회귀 모델 학습의 목표가 된다.

## 3. MSE의 수학적 속성과 직관적 의미

MSE는 왜 단순히 오차의 절댓값이 아닌, 번거롭게 '제곱'을 하는 방식을 사용할까? 여기에는 경사 하강법과 연계되는 중요한 수학적 속성과 직관적인 해석이 담겨 있다.

#### 3.1. 왜 오차를 제곱하는가?

오차를 제곱하는 데에는 두 가지 주요 이유가 있다.

1.  **오차 부호의 제거:** 예측이 실제보다 크면 오차는 양수(+)가 되고, 작으면 음수(-)가 된다. 만약 오차를 그대로 더하면 양수와 음수가 서로 상쇄되어 전체 오차가 0에 가까워지는 왜곡이 발생할 수 있다. 오차를 제곱함으로써 모든 오차는 양수가 되므로, 이러한 상쇄 효과를 방지하고 오차의 크기 자체에 집중할 수 있다.

2.  **큰 오차에 대한 불이익(Penalty) 부여:** 제곱 연산은 오차가 클수록 그 값을 훨씬 더 크게 만든다.
    * 오차가 `2`이면 제곱은 `4`가 된다.
    * 오차가 `10`이면 제곱은 `100`이 된다.
    이처럼, MSE를 손실 함수로 사용하면 모델은 예측이 크게 빗나간 데이터(Outlier)에 대해 훨씬 더 큰 불이익을 받게 된다. 따라서 모델은 이러한 '큰 실수'를 줄이는 방향으로 파라미터를 업데이트하게 된다.

#### 3.2. 경사 하강법과의 관계: 미분 가능성

손실 함수를 0에 가깝게 만들기 위해 우리는 경사 하강법(Gradient Descent)을 사용한다. 경사 하강법은 손실 함수를 미분하여 기울기를 구하고, 이 기울기를 따라 파라미터를 업데이트한다.

MSE를 $\theta$에 대해 그래프로 그리면 아래로 볼록한 **이차 함수(Quadratic function)** 형태를 띤다. 이 '그릇' 모양의 함수는 다음과 같은 중요한 특징을 가진다.

* **매끄럽게 이어진다 (Smooth):** 어느 지점에서나 미분이 가능하다.
* **전역 최솟값(Global Minimum)이 존재한다:** 단 하나의 가장 낮은 지점(최솟값)이 존재하여, 경사 하강법이 다른 곳으로 빠지지 않고 안정적으로 최적의 해를 찾을 수 있도록 보장한다.

만약 오차의 절댓값 합(Mean Absolute Error, MAE)을 사용하면 뾰족한 지점이 발생하여 미분이 특정 지점에서 불가능해지는 문제가 생길 수 있지만, MSE는 이러한 문제에서 자유롭다.

## 4. 결론

**평균 제곱 오차(MSE)**는 회귀 문제의 성능을 측정하는 가장 기본적인 척도이다. 오차를 **제곱**하여 평균을 내는 간단한 구조를 통해, 모델이 큰 실수를 하지 않도록 유도하며, 경사 하강법이 안정적으로 작동할 수 있는 매끄러운 손실 공간을 제공한다.

이러한 특성 덕분에 MSE는 오늘날에도 수많은 회귀 모델의 학습 과정에서 핵심적인 손실 함수로 사용되고 있다.