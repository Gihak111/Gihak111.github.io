---
layout: single
title:  "Gradient Explosion"
categories: "AI"
tag: "linear algebra"
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

## 경사 폭발

특히, RNN 학습시에 많이 일어나는 일 이다.  
경사 하강법에서 기울기(Gradient)는 모델이 나아갈 방향을 알려주는 '나침반'과 같은 핵심적인 역할을 한다.  
하지만 신경망이 깊어질수록, 특히 순환 신경망(RNN)처럼 동일한 가중치를 반복적으로 사용하는 구조에서는 이 나침반이 고장 나는 현상이 발생할 수 있다.  

이 문제는 양 극단의 형태로 나타나는데, 기울기가 기하급수적으로 커져버리는 **경사 폭발(Gradient Explosion)**과 반대로 거의 0에 가깝게 사라지는 **경사 소실(Gradient Vanishing)**이 나타난다.  

## 2. 경사 폭발

**경사 폭발**이란, 역전파(Backpropagation) 과정에서 계산되는 기울기 값이 계층(Layer)을 거슬러 올라가면서 눈덩이처럼 불어나 비정상적으로 커지는 현상을 말한다.  

역전파는 출력층의 오차로부터 시작해 입력층 방향으로 각 가중치에 대한 기울기를 계산한다.  
이 과정은 미분의 연쇄 법칙에 따라 이전 계층의 기울기에 현재 계층의 기울기를 계속해서 곱해나가는 방식으로 이루어진다.  

$L$개의 층으로 이루어진 단순한 신경망을 가정해보자.  
손실 함수 $J$에 대한 1번 층의 가중치 $W_1$의 기울기는 연쇄 법칙에 의해 다음과 같이 표현할 수 있다.  

$$\frac{\partial J}{\partial W_1} = \frac{\partial J}{\partial a_L} \frac{\partial a_L}{\partial a_{L-1}} \cdots \frac{\partial a_2}{\partial a_1} \frac{\partial a_1}{\partial W_1}$$

여기서 각 층의 연결 관계인 $\frac{\partial a_i}{\partial a_{i-1}}$ 항을 좀 더 자세히 살펴보면, 다음과 같이 가중치 행렬 $W_i$와 활성화 함수의 도함수 $\sigma'$의 곱으로 이루어져 있다.  

$$\frac{\partial a_i}{\partial a_{i-1}} = \frac{\partial a_i}{\partial z_i} \frac{\partial z_i}{\partial a_{i-1}} = \sigma'(z_i) W_i$$

결국, 초기 계층의 기울기는 여러 층에 걸친 가중치 행렬과 활성화 함수 도함수들의 곱으로 계산된다.  

$$\frac{\partial J}{\partial W_1} \propto \prod_{i=2}^{L} W_i \sigma'(z_i)$$

이때 두 가지 극단적인 상황이 발생할 수 있다.  

* **경사 폭발 (Explosion):** 만약 행렬 $W_i$의 값들이 1보다 크고, 활성화 함수 도함수 $\sigma'(z_i)$ 값도 커서 $(W_i \sigma'(z_i))$ 항의 크기가 반복적으로 1보다 커지면, 이 값들이 계속 곱해지면서 $\frac{\partial J}{\partial W_1}$는 기하급수적으로 커진다.  
* **경사 소실 (Vanishing):** 반대로 $(W_i \sigma'(z_i))$ 항의 크기가 반복적으로 1보다 작아지면, 전체 곱은 0에 가깝게 수렴하여 기울기가 사라진다. (예: Sigmoid 함수의 도함수 최댓값은 0.25로, 층이 깊어지면 기울기가 소실되기 쉽다.)  


이렇게 비정상적으로 커진 기울기는 가중치($W$)를 극단적으로 크게 업데이트한다.  
그 결과, 가중치 값들은 무한대(inf)나 숫자가 아님(NaN)을 의미하는 값으로 발산해버린다.  
이는 결국 모델이 더 이상 학습을 진행할 수 없는, 말 그대로 '폭발'하는 상태로 이어진다.  

## 3. 경사 폭발을 막기 위한 해결책

경사 폭발을 방지하고 안정적인 학습을 보장하기 위한 몇 가지 핵심적인 기법이 존재한다.  

1. 경사 클리핑 (Gradient Clipping)  
경사 폭발에 대한 가장 직접적이고 효과적인 해결책이다.  
* **원리**: 기울기의 크기에 **임계값(Threshold)**을 설정하고, 역전파 과정에서 계산된 기울기가 이 임계값을 초과하면 강제로 크기를 줄여주는 기법이다.  
* **작동 방식**: 기울기 벡터의 L2 노름(Norm, 벡터의 크기)을 계산한 뒤, 이 값이 임계값을 넘으면 기울기 벡터의 방향은 그대로 유지한 채 크기만 임계값에 맞춰 조절한다.  
마치 소리가 너무 커지면 자동으로 볼륨을 줄이는 오디오 리미터(limiter)와 같다.  
* **효과**: 기울기가 비정상적으로 커지는 것을 원천적으로 막아주어 학습의 안정성을 크게 높여준다.  

2. 가중치 초기화 (Weight Initialization)  
학습이 잘못된 방향으로 출발하지 않도록 막는 예방책이다.  
* **원리**: 학습 시작 시 가중치를 너무 크거나 작지 않은 적절한 값으로 초기화하여, 기울기가 처음부터 폭발하거나 소실될 가능성을 줄이는 방법이다.  
* **종류**: 대표적으로 **Xavier(Glorot) 초기화**나 **He 초기화** 기법이 사용된다.  
이 기법들은 계층의 입출력 노드 수를 고려하여 가중치의 초기 분산을 조절함으로써, 신호가 여러 층을 통과하더라도 안정적으로 유지되도록 돕는다.  

3. 배치 정규화 (Batch Normalization)  
학습 과정 자체를 안정시키는 방법이다.  
* **원리**: 각 계층을 통과한 출력값을 미니배치 단위로 정규화(평균 0, 분산 1)하는 기법이다.  
* **효과**: 각 층의 입력 데이터 분포를 안정시켜 학습을 원활하게 하고, 결과적으로 기울기가 너무 커지거나 작아지는 현상을 완화하는 데 도움을 준다.  

## 4. 결론  

**경사 폭발**은 깊은 신경망의 학습을 방해하는 심각한 문제이지만, 원인이 명확하고 효과적인 해결책이 존재한다.  
**경사 클리핑**을 통해 기울기의 폭주를 직접적으로 제어하고, **가중치 초기화**와 **배치 정규화**를 통해 학습 환경 자체를 안정적으로 만드는 것이 중요하다.  

이러한 기법들을 통해 기울기 문제를 효과적으로 제어함으로써, 오늘날 우리는 훨씬 더 깊고 복잡한 신경망도 안정적으로 학습시킬 수 있다.  
논문들도 조금 추천해 보자면,  
[기울기 소실 문제](https://arxiv.org/abs/1003.0389)  
[기울기 소실 문제 해결한 아키텍쳐](https://arxiv.org/abs/1512.03385)  
[뎁스의 중요성](https://arxiv.org/abs/1312.6098)  